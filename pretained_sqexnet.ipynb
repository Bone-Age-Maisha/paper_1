{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+WeLlBiRbgijATzKmbvrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bone-Age-Maisha/paper_1/blob/main/pretained_sqexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6OPE21n9ZaW",
        "outputId": "22c3b2ef-3f74-41f9-88c7-894d780b4c88"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "tLAXNcEW_WiG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_dir = '/content/drive/MyDrive/small_data/train'\n",
        "df = pd.read_csv('/content/drive/MyDrive/small_data/train_csv1.csv')"
      ],
      "metadata": {
        "id": "-Q29cr9ONQKk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "tf.config.experimental.set_memory_growth(gpus[0], True)"
      ],
      "metadata": {
        "id": "4qjgtSOqNSaR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_age = []\n",
        "y_gender = []\n",
        "\n",
        "#df = pd.read_csv('/raid/chenchao/code/BoneAge/BoneAge/data/Training.csv')\n",
        "a = df.values\n",
        "m = a.shape[0]\n",
        "\n",
        "path = train_dir\n",
        "k = 0\n",
        "print ('Loading data set...')\n",
        "k=1\n",
        "for i in os.listdir(path):\n",
        "  #print(i)\n",
        "  if(len(i)>9):   #errror occuring  so to \n",
        "    continue\n",
        "  y_age.append(df.boneage[df.id == int(i[:-4])].tolist()[0])\n",
        "  a = df.male[df.id == int(i[:-4])].tolist()[0]\n",
        "  if a:\n",
        "    y_gender.append(1)\n",
        "  else:\n",
        "     y_gender.append(0)\n",
        "  img_path = path + \"/\"+i\n",
        "  img = cv2.imread(img_path)\n",
        "  #print(img.shape)\n",
        "  #print (img_path)\n",
        "  img = cv2.imread(img_path)\n",
        "    #print (img_path)\n",
        "    #if(img is not None):\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img,(300,300))\n",
        "  x = np.asarray(img, dtype=np.uint8)\n",
        "  X_train.append(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "badbQuIXNW4i",
        "outputId": "ba4f9973-f0e8-4c9f-b356-9c86d9957d5c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data set...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softlabel(label,num_class):\n",
        "    softlabel=np.zeros((len(label),num_class))\n",
        "    ratio = 1.0/50\n",
        "    for i in range(len(label)):\n",
        "        for j in range(num_class):\n",
        "            softlabel[i,j]=1.0 - ratio*np.abs(j-label[i])\n",
        "    softlabel = np.maximum(softlabel,0)\n",
        "    return softlabel"
      ],
      "metadata": {
        "id": "FabdVRN0NbXv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.asarray(y_age)\n",
        "gender = np.asarray(y_gender)\n",
        "x=np.asarray(X_train, dtype=np.float32)\n",
        "x/255\n",
        "gender =2*( gender-0.5)\n",
        "x_final = []\n",
        "y_final = []\n",
        "gender_final = []\n",
        "\n",
        "# Shuffle images and split into train, validation and test sets\n",
        "#random_no = np.random.choice(x.shape[0], size=x.shape[0], replace=False)\n",
        "random_no = np.arange(x.shape[0])\n",
        "#print(random_no)\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(random_no)\n",
        "for i in random_no:\n",
        "    x_final.append(x[i,:,:,:])\n",
        "    y_final.append(y[i])\n",
        "    gender_final.append(gender[i])\n",
        "\n",
        "x_final = np.asarray(x_final)\n",
        "y_final = np.asarray(y_final)\n",
        "gender_final = np.asarray(gender_final)\n",
        "print (y_final[:50])\n",
        "print (gender_final[:50])\n",
        "k = 10 # Decides split count\n",
        "x_test = x_final[:k,:,:,:]\n",
        "y_test = y_final[:k]\n",
        "gender_test = gender_final[:k]\n",
        "x_valid = x_final[k:2*k,:,:,:]\n",
        "y_valid = y_final[k:2*k]\n",
        "gender_valid = gender_final[k:2*k]\n",
        "x_train = x_final[2*k:,:,:,:]\n",
        "y_train = y_final[2*k:]\n",
        "gender_train = gender_final[2*k:]\n",
        "\n",
        "## \n",
        "#y_test = keras.utils.to_categorical(y_test,240)\n",
        "#y_train = keras.utils.to_categorical(y_train,240)\n",
        "#y_valid = keras.utils.to_categorical(y_valid,240)\n",
        "y_train = softlabel(y_train,240)\n",
        "y_valid = softlabel(y_valid,240)\n",
        "y_test = softlabel(y_test,240)\n",
        "print (y_train)\n",
        "\n",
        "\n",
        "print ('x_train shape:'+ str(x_train.shape))\n",
        "print ('y_train shape:'+ str(y_train.shape))\n",
        "print ('gender_train shape:'+ str(gender_train.shape))\n",
        "print ('x_valid shape:'+ str(x_valid.shape))\n",
        "print ('y_valid shape:'+ str(y_valid.shape))\n",
        "print ('gender_valid shape:' + str(gender_valid.shape))\n",
        "print ('x_test shape:'+ str(x_test.shape))\n",
        "print ('y_test shape:'+ str(y_test.shape))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYqGmuVWNi5n",
        "outputId": "341797ff-726c-4370-c960-38608cbbabb8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 30 149 113 132  33 136 150  94  24  32  60 126  88 174  78  42  21  21\n",
            "  54  82 192  24  94  32 156 120 165  33 138 156  27 108  42 162  57 126\n",
            "   4 156 180  88  36 180 132 156 120  60  90 138 138 120]\n",
            "[-1.  1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1. -1.  1.  1.  1. -1.  1.\n",
            "  1. -1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1. -1.\n",
            " -1. -1.  1.  1. -1. -1. -1.  1. -1. -1.  1.  1.  1. -1.]\n",
            "[[0.   0.   0.   ... 0.1  0.08 0.06]\n",
            " [0.52 0.54 0.56 ... 0.   0.   0.  ]\n",
            " [0.   0.   0.   ... 0.   0.   0.  ]\n",
            " ...\n",
            " [0.   0.02 0.04 ... 0.   0.   0.  ]\n",
            " [0.   0.   0.   ... 0.   0.   0.  ]\n",
            " [0.   0.   0.   ... 0.   0.   0.  ]]\n",
            "x_train shape:(51, 300, 300, 3)\n",
            "y_train shape:(51, 240)\n",
            "gender_train shape:(51,)\n",
            "x_valid shape:(10, 300, 300, 3)\n",
            "y_valid shape:(10, 240)\n",
            "gender_valid shape:(10,)\n",
            "x_test shape:(10, 300, 300, 3)\n",
            "y_test shape:(10, 240)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install visualization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRjT86UJNlhx",
        "outputId": "55b41f98-8192-49eb-cf4c-2c6b05d141bf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: visualization in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from visualization) (3.2.2)\n",
            "Requirement already satisfied: pyrender in /usr/local/lib/python3.7/dist-packages (from visualization) (0.1.45)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from visualization) (1.21.6)\n",
            "Requirement already satisfied: trimesh[easy] in /usr/local/lib/python3.7/dist-packages (from visualization) (3.16.4)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from visualization) (2.9.0)\n",
            "Requirement already satisfied: autolab-core in /usr/local/lib/python3.7/dist-packages (from visualization) (1.1.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (1.7.3)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (0.17.21)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (4.6.0.66)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (0.70.14)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (1.3.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (1.2.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (6.7.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->visualization) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->visualization) (1.15.0)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.7/dist-packages (from multiprocess->autolab-core->visualization) (0.3.6)\n",
            "Requirement already satisfied: freetype-py in /usr/local/lib/python3.7/dist-packages (from pyrender->visualization) (2.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pyrender->visualization) (2.6.3)\n",
            "Requirement already satisfied: pyglet>=1.4.10 in /usr/local/lib/python3.7/dist-packages (from pyrender->visualization) (1.5.27)\n",
            "Requirement already satisfied: PyOpenGL==3.1.0 in /usr/local/lib/python3.7/dist-packages (from pyrender->visualization) (3.1.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml->autolab-core->visualization) (0.2.7)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->autolab-core->visualization) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->autolab-core->visualization) (2021.11.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autolab-core->visualization) (3.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (4.9.1)\n",
            "Requirement already satisfied: pycollada in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (0.7.2)\n",
            "Requirement already satisfied: rtree in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (57.4.0)\n",
            "Requirement already satisfied: mapbox-earcut in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.0.0)\n",
            "Requirement already satisfied: svg.path in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (6.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (3.1.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.8.5.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (2.23.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (3.0.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (4.3.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.0.4)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (0.19.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (5.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (4.13.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->trimesh[easy]->visualization) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->trimesh[easy]->visualization) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->trimesh[easy]->visualization) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->trimesh[easy]->visualization) (1.24.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->trimesh[easy]->visualization) (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.xception import Xception\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Flatten, Dense, Input, Reshape, Lambda\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "#from func_utils import *\n",
        "import os"
      ],
      "metadata": {
        "id": "27cgmDn1NpCD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model11 = ResNet50(weights='imagenet', include_top=False)\n",
        "base_model11.trainable = True\n",
        "print(\"Number of layers in the base model: \", len(base_model11.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model11.layers[:fine_tune_at]:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWg1RoZrOCMt",
        "outputId": "4e85db8f-2f62-4e24-9c16-b0cda519a166"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in the base model:  175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,layer in enumerate(base_model11.layers):\n",
        "    print (i,layer.name)\n",
        "input11 = Input(shape=(300,300,3),name='input1')\n",
        "input_gender11 = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output11 = base_model11(input11)\n",
        "gender_embedding11=Dense(16)(input_gender11)\n",
        "#gender_embedding=Dense(12)(gender_embedding)\n",
        "#x = keras.layers.MaxPooling2D(pool_size=(3,3))(output)\n",
        "#x = keras.layers.Conv2D(512,kernel_size=(3,3))(x)\n",
        "#x = keras.layers.Conv2D(256,kernel_size=(1,1))(x)\n",
        "print (K.int_shape(output11))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIBA72zFOPuF",
        "outputId": "113d9a19-eae3-46d7-d75f-f9b8517ff4ad"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_3\n",
            "1 conv1_pad\n",
            "2 conv1_conv\n",
            "3 conv1_bn\n",
            "4 conv1_relu\n",
            "5 pool1_pad\n",
            "6 pool1_pool\n",
            "7 conv2_block1_1_conv\n",
            "8 conv2_block1_1_bn\n",
            "9 conv2_block1_1_relu\n",
            "10 conv2_block1_2_conv\n",
            "11 conv2_block1_2_bn\n",
            "12 conv2_block1_2_relu\n",
            "13 conv2_block1_0_conv\n",
            "14 conv2_block1_3_conv\n",
            "15 conv2_block1_0_bn\n",
            "16 conv2_block1_3_bn\n",
            "17 conv2_block1_add\n",
            "18 conv2_block1_out\n",
            "19 conv2_block2_1_conv\n",
            "20 conv2_block2_1_bn\n",
            "21 conv2_block2_1_relu\n",
            "22 conv2_block2_2_conv\n",
            "23 conv2_block2_2_bn\n",
            "24 conv2_block2_2_relu\n",
            "25 conv2_block2_3_conv\n",
            "26 conv2_block2_3_bn\n",
            "27 conv2_block2_add\n",
            "28 conv2_block2_out\n",
            "29 conv2_block3_1_conv\n",
            "30 conv2_block3_1_bn\n",
            "31 conv2_block3_1_relu\n",
            "32 conv2_block3_2_conv\n",
            "33 conv2_block3_2_bn\n",
            "34 conv2_block3_2_relu\n",
            "35 conv2_block3_3_conv\n",
            "36 conv2_block3_3_bn\n",
            "37 conv2_block3_add\n",
            "38 conv2_block3_out\n",
            "39 conv3_block1_1_conv\n",
            "40 conv3_block1_1_bn\n",
            "41 conv3_block1_1_relu\n",
            "42 conv3_block1_2_conv\n",
            "43 conv3_block1_2_bn\n",
            "44 conv3_block1_2_relu\n",
            "45 conv3_block1_0_conv\n",
            "46 conv3_block1_3_conv\n",
            "47 conv3_block1_0_bn\n",
            "48 conv3_block1_3_bn\n",
            "49 conv3_block1_add\n",
            "50 conv3_block1_out\n",
            "51 conv3_block2_1_conv\n",
            "52 conv3_block2_1_bn\n",
            "53 conv3_block2_1_relu\n",
            "54 conv3_block2_2_conv\n",
            "55 conv3_block2_2_bn\n",
            "56 conv3_block2_2_relu\n",
            "57 conv3_block2_3_conv\n",
            "58 conv3_block2_3_bn\n",
            "59 conv3_block2_add\n",
            "60 conv3_block2_out\n",
            "61 conv3_block3_1_conv\n",
            "62 conv3_block3_1_bn\n",
            "63 conv3_block3_1_relu\n",
            "64 conv3_block3_2_conv\n",
            "65 conv3_block3_2_bn\n",
            "66 conv3_block3_2_relu\n",
            "67 conv3_block3_3_conv\n",
            "68 conv3_block3_3_bn\n",
            "69 conv3_block3_add\n",
            "70 conv3_block3_out\n",
            "71 conv3_block4_1_conv\n",
            "72 conv3_block4_1_bn\n",
            "73 conv3_block4_1_relu\n",
            "74 conv3_block4_2_conv\n",
            "75 conv3_block4_2_bn\n",
            "76 conv3_block4_2_relu\n",
            "77 conv3_block4_3_conv\n",
            "78 conv3_block4_3_bn\n",
            "79 conv3_block4_add\n",
            "80 conv3_block4_out\n",
            "81 conv4_block1_1_conv\n",
            "82 conv4_block1_1_bn\n",
            "83 conv4_block1_1_relu\n",
            "84 conv4_block1_2_conv\n",
            "85 conv4_block1_2_bn\n",
            "86 conv4_block1_2_relu\n",
            "87 conv4_block1_0_conv\n",
            "88 conv4_block1_3_conv\n",
            "89 conv4_block1_0_bn\n",
            "90 conv4_block1_3_bn\n",
            "91 conv4_block1_add\n",
            "92 conv4_block1_out\n",
            "93 conv4_block2_1_conv\n",
            "94 conv4_block2_1_bn\n",
            "95 conv4_block2_1_relu\n",
            "96 conv4_block2_2_conv\n",
            "97 conv4_block2_2_bn\n",
            "98 conv4_block2_2_relu\n",
            "99 conv4_block2_3_conv\n",
            "100 conv4_block2_3_bn\n",
            "101 conv4_block2_add\n",
            "102 conv4_block2_out\n",
            "103 conv4_block3_1_conv\n",
            "104 conv4_block3_1_bn\n",
            "105 conv4_block3_1_relu\n",
            "106 conv4_block3_2_conv\n",
            "107 conv4_block3_2_bn\n",
            "108 conv4_block3_2_relu\n",
            "109 conv4_block3_3_conv\n",
            "110 conv4_block3_3_bn\n",
            "111 conv4_block3_add\n",
            "112 conv4_block3_out\n",
            "113 conv4_block4_1_conv\n",
            "114 conv4_block4_1_bn\n",
            "115 conv4_block4_1_relu\n",
            "116 conv4_block4_2_conv\n",
            "117 conv4_block4_2_bn\n",
            "118 conv4_block4_2_relu\n",
            "119 conv4_block4_3_conv\n",
            "120 conv4_block4_3_bn\n",
            "121 conv4_block4_add\n",
            "122 conv4_block4_out\n",
            "123 conv4_block5_1_conv\n",
            "124 conv4_block5_1_bn\n",
            "125 conv4_block5_1_relu\n",
            "126 conv4_block5_2_conv\n",
            "127 conv4_block5_2_bn\n",
            "128 conv4_block5_2_relu\n",
            "129 conv4_block5_3_conv\n",
            "130 conv4_block5_3_bn\n",
            "131 conv4_block5_add\n",
            "132 conv4_block5_out\n",
            "133 conv4_block6_1_conv\n",
            "134 conv4_block6_1_bn\n",
            "135 conv4_block6_1_relu\n",
            "136 conv4_block6_2_conv\n",
            "137 conv4_block6_2_bn\n",
            "138 conv4_block6_2_relu\n",
            "139 conv4_block6_3_conv\n",
            "140 conv4_block6_3_bn\n",
            "141 conv4_block6_add\n",
            "142 conv4_block6_out\n",
            "143 conv5_block1_1_conv\n",
            "144 conv5_block1_1_bn\n",
            "145 conv5_block1_1_relu\n",
            "146 conv5_block1_2_conv\n",
            "147 conv5_block1_2_bn\n",
            "148 conv5_block1_2_relu\n",
            "149 conv5_block1_0_conv\n",
            "150 conv5_block1_3_conv\n",
            "151 conv5_block1_0_bn\n",
            "152 conv5_block1_3_bn\n",
            "153 conv5_block1_add\n",
            "154 conv5_block1_out\n",
            "155 conv5_block2_1_conv\n",
            "156 conv5_block2_1_bn\n",
            "157 conv5_block2_1_relu\n",
            "158 conv5_block2_2_conv\n",
            "159 conv5_block2_2_bn\n",
            "160 conv5_block2_2_relu\n",
            "161 conv5_block2_3_conv\n",
            "162 conv5_block2_3_bn\n",
            "163 conv5_block2_add\n",
            "164 conv5_block2_out\n",
            "165 conv5_block3_1_conv\n",
            "166 conv5_block3_1_bn\n",
            "167 conv5_block3_1_relu\n",
            "168 conv5_block3_2_conv\n",
            "169 conv5_block3_2_bn\n",
            "170 conv5_block3_2_relu\n",
            "171 conv5_block3_3_conv\n",
            "172 conv5_block3_3_bn\n",
            "173 conv5_block3_add\n",
            "174 conv5_block3_out\n",
            "(None, 10, 10, 2048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sq = keras.layers.GlobalAveragePooling2D()(output11)\n",
        "print (K.int_shape(sq))\n",
        "sq = keras.layers.Reshape((1,1,2048))(sq)\n",
        "sq = keras.layers.Dense(units=2048,activation=\"sigmoid\")(sq)\n",
        "block = keras.layers.multiply([output11,sq])\n",
        "#fine Squeeze and Excitation 1\n",
        "\n",
        "net = keras.layers.add([output11,block])\n",
        "net = keras.layers.BatchNormalization()(net)\n",
        "net = keras.layers.Activation(\"relu\")(net)\n",
        "net = keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"block_1\")(net)\n",
        "print (K.int_shape(net))\n",
        "\n",
        "x11 = keras.layers.MaxPooling2D(pool_size=(4,4))(net)\n",
        "print (K.int_shape(x11))\n",
        "x11=Flatten()(x11)\n",
        "f11 = keras.layers.Concatenate(axis=1)([x11,gender_embedding11])\n",
        "print (K.int_shape(f11)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "prediction11 = Dense(240)(f11)\n",
        "\n",
        "model11 = Model(inputs=[input11,input_gender11], outputs=prediction11)\n",
        "for i,layer in enumerate(model11.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0003,beta_1=0.9,beta_2=0.999)\n",
        "model11.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlZT6cFTOYfV",
        "outputId": "3e73102a-93b4-4b57-8030-2e13fcceeff4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 2048)\n",
            "(None, 5, 5, 2048)\n",
            "(None, 1, 1, 2048)\n",
            "(None, 2064)\n",
            "0 input1\n",
            "1 resnet50\n",
            "2 global_average_pooling2d_10\n",
            "3 reshape_10\n",
            "4 dense_14\n",
            "5 multiply_8\n",
            "6 add_7\n",
            "7 batch_normalization_5\n",
            "8 activation_5\n",
            "9 block_1\n",
            "10 max_pooling2d_5\n",
            "11 input2\n",
            "12 flatten_3\n",
            "13 dense_13\n",
            "14 concatenate_3\n",
            "15 dense_15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Adam=tf.keras.optimizers.Adam(lr=0.0001,beta_1=0.9,beta_2=0.999)\n",
        "model11.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])\n",
        "\n",
        "# Save weights after every epoch\n",
        "#dr='/content/drive/MyDrive/Colab Notebooks/weights'\n",
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "history11=model11.fit([x_train,gender_train],y_train,batch_size=32,epochs=200,verbose=1,validation_data=([x_valid,gender_valid],y_valid), callbacks = [checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9D6vbA1PMqJ",
        "outputId": "6c94733f-ab68-4887-c60c-e6b09f57914b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 6s 1s/step - loss: 4.8469 - MAE: 4.8469 - val_loss: 6.9152 - val_MAE: 6.9152\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 1s 283ms/step - loss: 2.7675 - MAE: 2.7675 - val_loss: 5.3097 - val_MAE: 5.3097\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 1.8542 - MAE: 1.8542 - val_loss: 4.4583 - val_MAE: 4.4583\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 1s 309ms/step - loss: 1.4027 - MAE: 1.4027 - val_loss: 3.9841 - val_MAE: 3.9841\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 1s 303ms/step - loss: 1.1404 - MAE: 1.1404 - val_loss: 3.6661 - val_MAE: 3.6661\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 1s 311ms/step - loss: 0.9286 - MAE: 0.9286 - val_loss: 3.4177 - val_MAE: 3.4177\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 0.8164 - MAE: 0.8164 - val_loss: 3.2224 - val_MAE: 3.2224\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 1s 405ms/step - loss: 0.6685 - MAE: 0.6685 - val_loss: 3.0638 - val_MAE: 3.0638\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 1s 449ms/step - loss: 0.5783 - MAE: 0.5783 - val_loss: 2.9202 - val_MAE: 2.9202\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 1s 321ms/step - loss: 0.4964 - MAE: 0.4964 - val_loss: 2.7873 - val_MAE: 2.7873\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 1s 301ms/step - loss: 0.4536 - MAE: 0.4536 - val_loss: 2.6628 - val_MAE: 2.6628\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.3902 - MAE: 0.3902 - val_loss: 2.5589 - val_MAE: 2.5589\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.3487 - MAE: 0.3487 - val_loss: 2.4599 - val_MAE: 2.4599\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.3249 - MAE: 0.3249 - val_loss: 2.3643 - val_MAE: 2.3643\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.2915 - MAE: 0.2915 - val_loss: 2.2718 - val_MAE: 2.2718\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 1s 282ms/step - loss: 0.2612 - MAE: 0.2612 - val_loss: 2.1875 - val_MAE: 2.1875\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.2508 - MAE: 0.2508 - val_loss: 2.1129 - val_MAE: 2.1129\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 1s 285ms/step - loss: 0.2230 - MAE: 0.2230 - val_loss: 2.0407 - val_MAE: 2.0407\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2102 - MAE: 0.2102 - val_loss: 1.9643 - val_MAE: 1.9643\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.1957 - MAE: 0.1957 - val_loss: 1.8927 - val_MAE: 1.8927\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.1905 - MAE: 0.1905 - val_loss: 1.8301 - val_MAE: 1.8301\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.1786 - MAE: 0.1786 - val_loss: 1.7778 - val_MAE: 1.7778\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.1687 - MAE: 0.1687 - val_loss: 1.7215 - val_MAE: 1.7215\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.1759 - MAE: 0.1759 - val_loss: 1.6611 - val_MAE: 1.6611\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 1s 312ms/step - loss: 0.1583 - MAE: 0.1583 - val_loss: 1.6087 - val_MAE: 1.6087\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.1562 - MAE: 0.1562 - val_loss: 1.5674 - val_MAE: 1.5674\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.1504 - MAE: 0.1504 - val_loss: 1.5294 - val_MAE: 1.5294\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.1454 - MAE: 0.1454 - val_loss: 1.4890 - val_MAE: 1.4890\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.1373 - MAE: 0.1373 - val_loss: 1.4457 - val_MAE: 1.4457\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 1s 721ms/step - loss: 0.1320 - MAE: 0.1320 - val_loss: 1.4026 - val_MAE: 1.4026\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.1289 - MAE: 0.1289 - val_loss: 1.3598 - val_MAE: 1.3598\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.1359 - MAE: 0.1359 - val_loss: 1.3239 - val_MAE: 1.3239\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.1234 - MAE: 0.1234 - val_loss: 1.2911 - val_MAE: 1.2911\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.1192 - MAE: 0.1192 - val_loss: 1.2592 - val_MAE: 1.2592\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.1269 - MAE: 0.1269 - val_loss: 1.2292 - val_MAE: 1.2292\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.1234 - MAE: 0.1234 - val_loss: 1.2002 - val_MAE: 1.2002\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.1123 - MAE: 0.1123 - val_loss: 1.1752 - val_MAE: 1.1752\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.1127 - MAE: 0.1127 - val_loss: 1.1528 - val_MAE: 1.1528\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.1067 - MAE: 0.1067 - val_loss: 1.1275 - val_MAE: 1.1275\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.1138 - MAE: 0.1138 - val_loss: 1.0982 - val_MAE: 1.0982\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.1044 - MAE: 0.1044 - val_loss: 1.0695 - val_MAE: 1.0695\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.1009 - MAE: 0.1009 - val_loss: 1.0361 - val_MAE: 1.0361\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0961 - MAE: 0.0961 - val_loss: 1.0039 - val_MAE: 1.0039\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.1121 - MAE: 0.1121 - val_loss: 0.9799 - val_MAE: 0.9799\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.0958 - MAE: 0.0958 - val_loss: 0.9579 - val_MAE: 0.9579\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0939 - MAE: 0.0939 - val_loss: 0.9385 - val_MAE: 0.9385\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.0934 - MAE: 0.0934 - val_loss: 0.9169 - val_MAE: 0.9169\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.1057 - MAE: 0.1057 - val_loss: 0.8968 - val_MAE: 0.8968\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0883 - MAE: 0.0883 - val_loss: 0.8787 - val_MAE: 0.8787\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0951 - MAE: 0.0951 - val_loss: 0.8602 - val_MAE: 0.8602\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0910 - MAE: 0.0910 - val_loss: 0.8405 - val_MAE: 0.8405\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0951 - MAE: 0.0951 - val_loss: 0.8210 - val_MAE: 0.8210\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0866 - MAE: 0.0866 - val_loss: 0.8036 - val_MAE: 0.8036\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0845 - MAE: 0.0845 - val_loss: 0.7889 - val_MAE: 0.7889\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0803 - MAE: 0.0803 - val_loss: 0.7768 - val_MAE: 0.7768\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 1s 306ms/step - loss: 0.0839 - MAE: 0.0839 - val_loss: 0.7627 - val_MAE: 0.7627\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.0821 - MAE: 0.0821 - val_loss: 0.7489 - val_MAE: 0.7489\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.0842 - MAE: 0.0842 - val_loss: 0.7346 - val_MAE: 0.7346\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.0845 - MAE: 0.0845 - val_loss: 0.7221 - val_MAE: 0.7221\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 1s 651ms/step - loss: 0.0862 - MAE: 0.0862 - val_loss: 0.7103 - val_MAE: 0.7103\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0797 - MAE: 0.0797 - val_loss: 0.6989 - val_MAE: 0.6989\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0828 - MAE: 0.0828 - val_loss: 0.6872 - val_MAE: 0.6872\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.0764 - MAE: 0.0764 - val_loss: 0.6745 - val_MAE: 0.6745\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0762 - MAE: 0.0762 - val_loss: 0.6618 - val_MAE: 0.6618\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0746 - MAE: 0.0746 - val_loss: 0.6495 - val_MAE: 0.6495\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0730 - MAE: 0.0730 - val_loss: 0.6394 - val_MAE: 0.6394\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.0791 - MAE: 0.0791 - val_loss: 0.6286 - val_MAE: 0.6286\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0735 - MAE: 0.0735 - val_loss: 0.6199 - val_MAE: 0.6199\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.0696 - MAE: 0.0696 - val_loss: 0.6129 - val_MAE: 0.6129\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0728 - MAE: 0.0728 - val_loss: 0.6055 - val_MAE: 0.6055\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 1s 305ms/step - loss: 0.0681 - MAE: 0.0681 - val_loss: 0.5960 - val_MAE: 0.5960\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0702 - MAE: 0.0702 - val_loss: 0.5868 - val_MAE: 0.5868\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0711 - MAE: 0.0711 - val_loss: 0.5782 - val_MAE: 0.5782\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0656 - MAE: 0.0656 - val_loss: 0.5693 - val_MAE: 0.5693\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0704 - MAE: 0.0704 - val_loss: 0.5602 - val_MAE: 0.5602\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0695 - MAE: 0.0695 - val_loss: 0.5526 - val_MAE: 0.5526\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.0672 - MAE: 0.0672 - val_loss: 0.5451 - val_MAE: 0.5451\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0615 - MAE: 0.0615 - val_loss: 0.5396 - val_MAE: 0.5396\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.0680 - MAE: 0.0680 - val_loss: 0.5348 - val_MAE: 0.5348\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.0632 - MAE: 0.0632 - val_loss: 0.5288 - val_MAE: 0.5288\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.0705 - MAE: 0.0705 - val_loss: 0.5227 - val_MAE: 0.5227\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0627 - MAE: 0.0627 - val_loss: 0.5167 - val_MAE: 0.5167\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0654 - MAE: 0.0654 - val_loss: 0.5098 - val_MAE: 0.5098\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0682 - MAE: 0.0682 - val_loss: 0.5016 - val_MAE: 0.5016\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0622 - MAE: 0.0622 - val_loss: 0.4949 - val_MAE: 0.4949\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.0661 - MAE: 0.0661 - val_loss: 0.4901 - val_MAE: 0.4901\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0596 - MAE: 0.0596 - val_loss: 0.4879 - val_MAE: 0.4879\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0595 - MAE: 0.0595 - val_loss: 0.4837 - val_MAE: 0.4837\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 1s 307ms/step - loss: 0.0641 - MAE: 0.0641 - val_loss: 0.4772 - val_MAE: 0.4772\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 1s 780ms/step - loss: 0.0678 - MAE: 0.0678 - val_loss: 0.4711 - val_MAE: 0.4711\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 1s 331ms/step - loss: 0.0622 - MAE: 0.0622 - val_loss: 0.4659 - val_MAE: 0.4659\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.0620 - MAE: 0.0620 - val_loss: 0.4606 - val_MAE: 0.4606\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 1s 312ms/step - loss: 0.0593 - MAE: 0.0593 - val_loss: 0.4553 - val_MAE: 0.4553\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 1s 304ms/step - loss: 0.0591 - MAE: 0.0591 - val_loss: 0.4504 - val_MAE: 0.4504\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 1s 303ms/step - loss: 0.0648 - MAE: 0.0648 - val_loss: 0.4464 - val_MAE: 0.4464\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 1s 309ms/step - loss: 0.0589 - MAE: 0.0589 - val_loss: 0.4431 - val_MAE: 0.4431\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0559 - MAE: 0.0559 - val_loss: 0.4390 - val_MAE: 0.4390\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 1s 311ms/step - loss: 0.0538 - MAE: 0.0538 - val_loss: 0.4341 - val_MAE: 0.4341\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0629 - MAE: 0.0629 - val_loss: 0.4294 - val_MAE: 0.4294\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0526 - MAE: 0.0526 - val_loss: 0.4267 - val_MAE: 0.4267\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0527 - MAE: 0.0527 - val_loss: 0.4241 - val_MAE: 0.4241\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0570 - MAE: 0.0570 - val_loss: 0.4201 - val_MAE: 0.4201\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0547 - MAE: 0.0547 - val_loss: 0.4150 - val_MAE: 0.4150\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0514 - MAE: 0.0514 - val_loss: 0.4119 - val_MAE: 0.4119\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0528 - MAE: 0.0528 - val_loss: 0.4116 - val_MAE: 0.4116\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.0501 - MAE: 0.0501 - val_loss: 0.4096 - val_MAE: 0.4096\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.0503 - MAE: 0.0503 - val_loss: 0.4055 - val_MAE: 0.4055\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0510 - MAE: 0.0510 - val_loss: 0.4026 - val_MAE: 0.4026\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.0496 - MAE: 0.0496 - val_loss: 0.4008 - val_MAE: 0.4008\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0520 - MAE: 0.0520 - val_loss: 0.3969 - val_MAE: 0.3969\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0546 - MAE: 0.0546 - val_loss: 0.3926 - val_MAE: 0.3926\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0534 - MAE: 0.0534 - val_loss: 0.3893 - val_MAE: 0.3893\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0524 - MAE: 0.0524 - val_loss: 0.3868 - val_MAE: 0.3868\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0530 - MAE: 0.0530 - val_loss: 0.3845 - val_MAE: 0.3845\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0500 - MAE: 0.0500 - val_loss: 0.3824 - val_MAE: 0.3824\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0543 - MAE: 0.0543 - val_loss: 0.3801 - val_MAE: 0.3801\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0533 - MAE: 0.0533 - val_loss: 0.3760 - val_MAE: 0.3760\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0520 - MAE: 0.0520 - val_loss: 0.3725 - val_MAE: 0.3725\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0514 - MAE: 0.0514 - val_loss: 0.3706 - val_MAE: 0.3706\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 1s 637ms/step - loss: 0.0539 - MAE: 0.0539 - val_loss: 0.3687 - val_MAE: 0.3687\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.0480 - MAE: 0.0480 - val_loss: 0.3671 - val_MAE: 0.3671\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0489 - MAE: 0.0489 - val_loss: 0.3660 - val_MAE: 0.3660\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0534 - MAE: 0.0534 - val_loss: 0.3645 - val_MAE: 0.3645\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.0473 - MAE: 0.0473 - val_loss: 0.3620 - val_MAE: 0.3620\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0471 - MAE: 0.0471 - val_loss: 0.3595 - val_MAE: 0.3595\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0443 - MAE: 0.0443 - val_loss: 0.3569 - val_MAE: 0.3569\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0478 - MAE: 0.0478 - val_loss: 0.3555 - val_MAE: 0.3555\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0464 - MAE: 0.0464 - val_loss: 0.3544 - val_MAE: 0.3544\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.0435 - MAE: 0.0435 - val_loss: 0.3523 - val_MAE: 0.3523\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0440 - MAE: 0.0440 - val_loss: 0.3494 - val_MAE: 0.3494\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0456 - MAE: 0.0456 - val_loss: 0.3473 - val_MAE: 0.3473\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.0468 - MAE: 0.0468 - val_loss: 0.3456 - val_MAE: 0.3456\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0476 - MAE: 0.0476 - val_loss: 0.3445 - val_MAE: 0.3445\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0498 - MAE: 0.0498 - val_loss: 0.3433 - val_MAE: 0.3433\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0490 - MAE: 0.0490 - val_loss: 0.3422 - val_MAE: 0.3422\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0441 - MAE: 0.0441 - val_loss: 0.3406 - val_MAE: 0.3406\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.0467 - MAE: 0.0467 - val_loss: 0.3381 - val_MAE: 0.3381\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.0484 - MAE: 0.0484 - val_loss: 0.3362 - val_MAE: 0.3362\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0437 - MAE: 0.0437 - val_loss: 0.3345 - val_MAE: 0.3345\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0515 - MAE: 0.0515 - val_loss: 0.3325 - val_MAE: 0.3325\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0491 - MAE: 0.0491 - val_loss: 0.3306 - val_MAE: 0.3306\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0466 - MAE: 0.0466 - val_loss: 0.3287 - val_MAE: 0.3287\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0451 - MAE: 0.0451 - val_loss: 0.3277 - val_MAE: 0.3277\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0452 - MAE: 0.0452 - val_loss: 0.3268 - val_MAE: 0.3268\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0425 - MAE: 0.0425 - val_loss: 0.3258 - val_MAE: 0.3258\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0419 - MAE: 0.0419 - val_loss: 0.3251 - val_MAE: 0.3251\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.0418 - MAE: 0.0418 - val_loss: 0.3242 - val_MAE: 0.3242\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.0414 - MAE: 0.0414 - val_loss: 0.3228 - val_MAE: 0.3228\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0442 - MAE: 0.0442 - val_loss: 0.3217 - val_MAE: 0.3217\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 1s 671ms/step - loss: 0.0398 - MAE: 0.0398 - val_loss: 0.3205 - val_MAE: 0.3205\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0465 - MAE: 0.0465 - val_loss: 0.3194 - val_MAE: 0.3194\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.0437 - MAE: 0.0437 - val_loss: 0.3183 - val_MAE: 0.3183\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0439 - MAE: 0.0439 - val_loss: 0.3177 - val_MAE: 0.3177\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0416 - MAE: 0.0416 - val_loss: 0.3169 - val_MAE: 0.3169\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0424 - MAE: 0.0424 - val_loss: 0.3160 - val_MAE: 0.3160\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0435 - MAE: 0.0435 - val_loss: 0.3158 - val_MAE: 0.3158\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.0393 - MAE: 0.0393 - val_loss: 0.3150 - val_MAE: 0.3150\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0379 - MAE: 0.0379 - val_loss: 0.3142 - val_MAE: 0.3142\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0392 - MAE: 0.0392 - val_loss: 0.3127 - val_MAE: 0.3127\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0394 - MAE: 0.0394 - val_loss: 0.3114 - val_MAE: 0.3114\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0392 - MAE: 0.0392 - val_loss: 0.3103 - val_MAE: 0.3103\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.0386 - MAE: 0.0386 - val_loss: 0.3095 - val_MAE: 0.3095\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0401 - MAE: 0.0401 - val_loss: 0.3086 - val_MAE: 0.3086\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0388 - MAE: 0.0388 - val_loss: 0.3079 - val_MAE: 0.3079\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0411 - MAE: 0.0411 - val_loss: 0.3075 - val_MAE: 0.3075\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0398 - MAE: 0.0398 - val_loss: 0.3069 - val_MAE: 0.3069\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.0430 - MAE: 0.0430 - val_loss: 0.3057 - val_MAE: 0.3057\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.0435 - MAE: 0.0435 - val_loss: 0.3044 - val_MAE: 0.3044\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0430 - MAE: 0.0430 - val_loss: 0.3032 - val_MAE: 0.3032\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0399 - MAE: 0.0399 - val_loss: 0.3021 - val_MAE: 0.3021\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0435 - MAE: 0.0435 - val_loss: 0.3013 - val_MAE: 0.3013\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0391 - MAE: 0.0391 - val_loss: 0.3002 - val_MAE: 0.3002\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0372 - MAE: 0.0372 - val_loss: 0.3000 - val_MAE: 0.3000\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0377 - MAE: 0.0377 - val_loss: 0.2993 - val_MAE: 0.2993\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.0352 - MAE: 0.0352 - val_loss: 0.2980 - val_MAE: 0.2980\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0400 - MAE: 0.0400 - val_loss: 0.2965 - val_MAE: 0.2965\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.0405 - MAE: 0.0405 - val_loss: 0.2955 - val_MAE: 0.2955\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 1s 310ms/step - loss: 0.0379 - MAE: 0.0379 - val_loss: 0.2953 - val_MAE: 0.2953\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 0.0402 - MAE: 0.0402 - val_loss: 0.2949 - val_MAE: 0.2949\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 1s 845ms/step - loss: 0.0383 - MAE: 0.0383 - val_loss: 0.2941 - val_MAE: 0.2941\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 1s 309ms/step - loss: 0.0466 - MAE: 0.0466 - val_loss: 0.2937 - val_MAE: 0.2937\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 1s 306ms/step - loss: 0.0403 - MAE: 0.0403 - val_loss: 0.2932 - val_MAE: 0.2932\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 1s 304ms/step - loss: 0.0502 - MAE: 0.0502 - val_loss: 0.2929 - val_MAE: 0.2929\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 1s 307ms/step - loss: 0.0403 - MAE: 0.0403 - val_loss: 0.2923 - val_MAE: 0.2923\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0404 - MAE: 0.0404 - val_loss: 0.2917 - val_MAE: 0.2917\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0383 - MAE: 0.0383 - val_loss: 0.2911 - val_MAE: 0.2911\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 1s 297ms/step - loss: 0.0367 - MAE: 0.0367 - val_loss: 0.2898 - val_MAE: 0.2898\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0371 - MAE: 0.0371 - val_loss: 0.2888 - val_MAE: 0.2888\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.0363 - MAE: 0.0363 - val_loss: 0.2884 - val_MAE: 0.2884\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.0385 - MAE: 0.0385 - val_loss: 0.2882 - val_MAE: 0.2882\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0373 - MAE: 0.0373 - val_loss: 0.2878 - val_MAE: 0.2878\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.0354 - MAE: 0.0354 - val_loss: 0.2880 - val_MAE: 0.2880\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0372 - MAE: 0.0372 - val_loss: 0.2879 - val_MAE: 0.2879\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0356 - MAE: 0.0356 - val_loss: 0.2865 - val_MAE: 0.2865\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.0435 - MAE: 0.0435 - val_loss: 0.2860 - val_MAE: 0.2860\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0366 - MAE: 0.0366 - val_loss: 0.2854 - val_MAE: 0.2854\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.0368 - MAE: 0.0368 - val_loss: 0.2851 - val_MAE: 0.2851\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.0364 - MAE: 0.0364 - val_loss: 0.2850 - val_MAE: 0.2850\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0355 - MAE: 0.0355 - val_loss: 0.2846 - val_MAE: 0.2846\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.0346 - MAE: 0.0346 - val_loss: 0.2838 - val_MAE: 0.2838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history11.history['loss'])\n",
        "plt.plot(history11.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "-rFgwKZeRfUO",
        "outputId": "0e448133-9571-4338-be16-83824e6c6d59"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "WpPCyUlORunV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmi8CKacbFSy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyu0HMaK9eZRUWzEGqHowf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bone-Age-Maisha/paper_1/blob/main/unfreez_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brqw91CtDPTk",
        "outputId": "7f5f3063-ace3-4030-cfdd-d6a1b445d201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "from six.moves import cPickle"
      ],
      "metadata": {
        "id": "QF6hGxokDTbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_dir = '/content/drive/MyDrive/small_data/train'\n",
        "df = pd.read_csv('/content/drive/MyDrive/small_data/train_csv1.csv')"
      ],
      "metadata": {
        "id": "SQixoiQUDwrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "tf.config.experimental.set_memory_growth(gpus[0], True)"
      ],
      "metadata": {
        "id": "4ialgi5cD5Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_age = []\n",
        "y_gender = []\n",
        "\n",
        "#df = pd.read_csv('/raid/chenchao/code/BoneAge/BoneAge/data/Training.csv')\n",
        "a = df.values\n",
        "m = a.shape[0]\n",
        "\n",
        "path = train_dir\n",
        "k = 0\n",
        "print ('Loading data set...')\n",
        "k=1\n",
        "for i in os.listdir(path):\n",
        "  print(i)\n",
        "  if(len(i)>9):   #errror occuring  so to \n",
        "    continue\n",
        "  y_age.append(df.boneage[df.id == int(i[:-4])].tolist()[0])\n",
        "  a = df.male[df.id == int(i[:-4])].tolist()[0]\n",
        "  if a:\n",
        "    y_gender.append(1)\n",
        "  else:\n",
        "     y_gender.append(0)\n",
        "  img_path = path + \"/\"+i\n",
        "  img = cv2.imread(img_path)\n",
        "  #print(img.shape)\n",
        "  #print (img_path)\n",
        "  img = cv2.imread(img_path)\n",
        "    #print (img_path)\n",
        "    #if(img is not None):\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img,(300,300))\n",
        "  x = np.asarray(img, dtype=np.uint8)\n",
        "  X_train.append(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DswN6oZEQj2",
        "outputId": "88ba8c35-b672-41c0-f5f8-14d3c028e1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data set...\n",
            "1378.png\n",
            "1377.png\n",
            "1379.png\n",
            "1380.png\n",
            "1381.png\n",
            "1385.png\n",
            "1383.png\n",
            "1384.png\n",
            "1382.png\n",
            "1387.png\n",
            "1388.png\n",
            "1390.png\n",
            "1391.png\n",
            "1389.png\n",
            "1395.png\n",
            "1393.png\n",
            "1394.png\n",
            "1399.png\n",
            "1398.png\n",
            "1396.png\n",
            "1402.png\n",
            "1403.png\n",
            "1400.png\n",
            "1406.png\n",
            "1405.png\n",
            "1404.png\n",
            "1408.png\n",
            "1409.png\n",
            "1407.png\n",
            "1411.png\n",
            "1412.png\n",
            "1414.png\n",
            "1416.png\n",
            "1415.png\n",
            "1419.png\n",
            "1418.png\n",
            "1417.png\n",
            "1420.png\n",
            "1423.png\n",
            "1424.png\n",
            "1422.png\n",
            "1426.png\n",
            "1427.png\n",
            "1425.png\n",
            "1429.png\n",
            "1428.png\n",
            "1430.png\n",
            "1433.png\n",
            "1434.png\n",
            "1431.png\n",
            "1432.png\n",
            "1435.png\n",
            "1437.png\n",
            "1436.png\n",
            "1439.png\n",
            "1438.png\n",
            "1440.png\n",
            "1443.png\n",
            "1442.png\n",
            "1441.png\n",
            "1445.png\n",
            "1444.png\n",
            "1446.png\n",
            "1447.png\n",
            "1448.png\n",
            "1453.png\n",
            "1451.png\n",
            "1452.png\n",
            "1454.png\n",
            "1455.png\n",
            "1457.png\n",
            ".ipynb_checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softlabel(label,num_class):\n",
        "    softlabel=np.zeros((len(label),num_class))\n",
        "    ratio = 1.0/50\n",
        "    for i in range(len(label)):\n",
        "        for j in range(num_class):\n",
        "            softlabel[i,j]=1.0 - ratio*np.abs(j-label[i])\n",
        "    softlabel = np.maximum(softlabel,0)\n",
        "    return softlabel"
      ],
      "metadata": {
        "id": "QtpzEEHhFlHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.asarray(y_age)\n",
        "gender = np.asarray(y_gender)\n",
        "x=np.asarray(X_train, dtype=np.float32)\n",
        "x/255\n",
        "gender =2*( gender-0.5)\n",
        "x_final = []\n",
        "y_final = []\n",
        "gender_final = []\n",
        "\n",
        "# Shuffle images and split into train, validation and test sets\n",
        "#random_no = np.random.choice(x.shape[0], size=x.shape[0], replace=False)\n",
        "random_no = np.arange(x.shape[0])\n",
        "#print(random_no)\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(random_no)\n",
        "for i in random_no:\n",
        "    x_final.append(x[i,:,:,:])\n",
        "    y_final.append(y[i])\n",
        "    gender_final.append(gender[i])\n",
        "\n",
        "x_final = np.asarray(x_final)\n",
        "y_final = np.asarray(y_final)\n",
        "gender_final = np.asarray(gender_final)\n",
        "print (y_final[:50])\n",
        "print (gender_final[:50])\n",
        "k = 10 # Decides split count\n",
        "x_test = x_final[:k,:,:,:]\n",
        "y_test = y_final[:k]\n",
        "gender_test = gender_final[:k]\n",
        "x_valid = x_final[k:2*k,:,:,:]\n",
        "y_valid = y_final[k:2*k]\n",
        "gender_valid = gender_final[k:2*k]\n",
        "x_train = x_final[2*k:,:,:,:]\n",
        "y_train = y_final[2*k:]\n",
        "gender_train = gender_final[2*k:]\n",
        "\n",
        "## \n",
        "#y_test = keras.utils.to_categorical(y_test,240)\n",
        "#y_train = keras.utils.to_categorical(y_train,240)\n",
        "#y_valid = keras.utils.to_categorical(y_valid,240)\n",
        "y_train = softlabel(y_train,240)\n",
        "y_valid = softlabel(y_valid,240)\n",
        "y_test = softlabel(y_test,240)\n",
        "print (y_train)\n",
        "\n",
        "\n",
        "print ('x_train shape:'+ str(x_train.shape))\n",
        "print ('y_train shape:'+ str(y_train.shape))\n",
        "print ('gender_train shape:'+ str(gender_train.shape))\n",
        "print ('x_valid shape:'+ str(x_valid.shape))\n",
        "print ('y_valid shape:'+ str(y_valid.shape))\n",
        "print ('gender_valid shape:' + str(gender_valid.shape))\n",
        "print ('x_test shape:'+ str(x_test.shape))\n",
        "print ('y_test shape:'+ str(y_test.shape))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BQ3h86rEhUE",
        "outputId": "ab4a2f03-046a-4b78-c32c-5cd3f7a8dabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[126 149 113 132 156 180 156  42 126 126  42  30  78 174  88 165  32 132\n",
            " 156  82 192 170  94  32 156 120  60  33 126  54  27 108  94 162 120  21\n",
            " 188  33 136  24   4  12 132  36  57  24  90 138 138 159]\n",
            "[ 1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
            "  1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1. -1.\n",
            " -1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1.  1.  1. -1.]\n",
            "[[0.   0.   0.   ... 0.1  0.08 0.06]\n",
            " [0.   0.   0.   ... 0.   0.   0.  ]\n",
            " [0.   0.   0.   ... 0.   0.   0.  ]\n",
            " ...\n",
            " [0.   0.02 0.04 ... 0.   0.   0.  ]\n",
            " [0.52 0.54 0.56 ... 0.   0.   0.  ]\n",
            " [0.   0.   0.   ... 0.   0.   0.  ]]\n",
            "x_train shape:(51, 300, 300, 3)\n",
            "y_train shape:(51, 240)\n",
            "gender_train shape:(51,)\n",
            "x_valid shape:(10, 300, 300, 3)\n",
            "y_valid shape:(10, 240)\n",
            "gender_valid shape:(10,)\n",
            "x_test shape:(10, 300, 300, 3)\n",
            "y_test shape:(10, 240)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from visualization import *\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "import os\n",
        "\n",
        "\n",
        "def ShowAttentionV1(model,image_path):\n",
        "    file_list = os.listdir(image_path)\n",
        "    file_list.sort()\n",
        "    for filename in file_list:\n",
        "        print (filename)\n",
        "        filepath=image_path+filename\n",
        "        image=load_image(filepath)\n",
        "        image = image/255.0\n",
        "        gender=1.0\n",
        "        gender=np.asarray(gender)\n",
        "        gender=np.expand_dims(gender,axis=0)\n",
        "        layer=K.function([model.layers[0].input],[model.layers[196].output])\n",
        "        FeatureMap=layer([image,gender])[0]\n",
        "        print (FeatureMap.shape)\n",
        "        FeatureMap = np.squeeze(FeatureMap, axis=0)\n",
        "        FeatureMap = np.abs(FeatureMap)\n",
        "        heatmap = np.mean(FeatureMap,axis=2)\n",
        "        heatmap = heatmap/np.max(heatmap)\n",
        "        heatmap = np.uint8(255*heatmap)\n",
        "        print (heatmap.shape)\n",
        "        heatmap = cv2.applyColorMap(heatmap,cv2.COLORMAP_JET)\n",
        "        SaveImg(filename,filepath,heatmap)\n",
        "    print ('********** Done ***********')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def GAPAttention(model,weights,image_path):\n",
        "    file_list = os.listdir(image_path)\n",
        "    file_list.sort()\n",
        "    for filename in file_list:\n",
        "        filepath=image_path+filename\n",
        "        print (filepath)\n",
        "        image=load_image(filepath)\n",
        "        print(\"gpa\")\n",
        "        print(image.shape)\n",
        "        image = image/255.0\n",
        "        gender=1.0\n",
        "        gender=np.asarray(gender)\n",
        "        gender=np.expand_dims(gender,axis=0)\n",
        "        print(\"ok\")\n",
        "        layer=K.function([model.layers[0].input],[model.layers[1].get_output_at(-1),model.layers[-1].output])\n",
        "        print(\"ok_1\")\n",
        "        GAP,prediction=layer([image])\n",
        "        print(\"ok_2\")\n",
        "        GAP=np.squeeze(GAP,axis=0)\n",
        "        print(\"ok_3\")\n",
        "        print (GAP.shape)\n",
        "        print(\"ok_4\")\n",
        "        index = np.argmax(prediction)\n",
        "        print(\"ok_5\")\n",
        "        print (index)\n",
        "       # weight = weights[:,index]\n",
        "        weight =np.mean(weights[:,index-5:index+5],axis=1)\n",
        "        heatmap = np.zeros((GAP.shape[0],GAP.shape[1]))\n",
        "        for k in range(GAP.shape[2]):\n",
        "            heatmap = heatmap + weight[k]*GAP[:,:,k]\n",
        "        heatmap = heatmap/np.max(heatmap)\n",
        "        heatmap = np.uint8(255*heatmap)\n",
        "        print (heatmap.shape)\n",
        "        heatmap = cv2.applyColorMap(heatmap,cv2.COLORMAP_JET)\n",
        "        SaveImg(filename,filepath,heatmap)\n",
        "    print ('********** Done ***********')\n",
        "\n",
        "\n",
        "\n",
        "def SaveImg(filename,filepath,heatmap):\n",
        "    img = cv2.imread(filepath)\n",
        "    heatmap = cv2.resize(heatmap,(img.shape[1],img.shape[0]))\n",
        "    AttentionImg =0.5* heatmap + img\n",
        "    cv2.imwrite('/content/heat'+filename,heatmap)\n",
        "    cv2.imwrite('/content/attention'+filename,AttentionImg)\n",
        "\n",
        "\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path)\n",
        "    print(img.shape)\n",
        "    img = cv2.resize(img,(300,300))\n",
        "    print(img.shape)\n",
        "    x = np.asarray(img, dtype=np.float32)\n",
        "   # img = image.load_img(path, target_size=(448, 448))\n",
        "   # print (img.shape)\n",
        "   # x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    return x\n",
        "\n",
        "def TestMAE(model,test_data,test_label,test_gender):\n",
        "    test_gender = np.array(test_gender)\n",
        "    test_gender = np.expand_dims(test_gender,axis=1)\n",
        "    layer=K.function([model.layers[0].input,model.layers[3].input],[model.layers[-1].output])\n",
        "    predictions=layer([test_data,test_gender])\n",
        "    predictions = np.array(predictions)\n",
        "    predictions = np.squeeze(predictions,axis=0)\n",
        "    print (predictions.shape)\n",
        "    predict_label = np.argmax(predictions,axis=1)\n",
        "    test_label = np.argmax(test_label,axis=1)\n",
        "    print (predict_label)\n",
        "    print (test_label)\n",
        "    TestMAE = np.mean(np.abs(predict_label-test_label))\n",
        "    return TestMAE\n",
        "\n",
        "\n",
        "\n",
        "def DataAugment(x_train):\n",
        "    x_train_Aug = np.zeros(x_train.shape)\n",
        "    for i in range(x_train.shape[0]):\n",
        "        for j in range(3):\n",
        "            img = x_train[i,:,:,j]\n",
        "            img = RandomMask(img)\n",
        "            img = RandomMask(img)\n",
        "            if np.random.random()>-1:\n",
        "                x_train_Aug[i,:,:,j]=img \n",
        "            else:\n",
        "                x_train_Aug[i,:,:,j]=x_train[i,:,:,j]\n",
        "    return x_train_Aug\n",
        "\n",
        "\n",
        "def RandomMask(img):\n",
        "    m,n=img.shape\n",
        "    m=int(m/6)\n",
        "    n=int(n/6)\n",
        "    i,j = np.random.randint(0,6,2)\n",
        "    img[i*m:(i+1)*m,j*n:(j+1)*n]=np.random.random()\n",
        "    return img\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHqcmR4gGB2B",
        "outputId": "addbbe0b-b367-4641-bb7c-83bd36651b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to import geometry msgs in rigid_transformations.py.\n",
            "WARNING:root:Failed to import ros dependencies in rigid_transforms.py\n",
            "WARNING:root:autolab_core not installed as catkin package, RigidTransform ros methods will be unavailable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install visualization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCO4COH9vac-",
        "outputId": "6ccd6d5b-f7f5-4eb2-8708-0d78edbccdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting visualization\n",
            "  Downloading visualization-1.0.0-py3-none-any.whl (13 kB)\n",
            "Collecting pyrender\n",
            "  Downloading pyrender-0.1.45-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 6.1 MB/s \n",
            "\u001b[?25hCollecting autolab-core\n",
            "  Downloading autolab_core-1.1.1-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 73.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from visualization) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from visualization) (3.2.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from visualization) (2.9.0)\n",
            "Collecting trimesh[easy]\n",
            "  Downloading trimesh-3.16.4-py3-none-any.whl (663 kB)\n",
            "\u001b[K     |████████████████████████████████| 663 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (0.18.3)\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (4.6.0.66)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (1.7.3)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 76.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (1.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->visualization) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->visualization) (1.15.0)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.7/dist-packages (from multiprocess->autolab-core->visualization) (0.3.6)\n",
            "Collecting pyglet>=1.4.10\n",
            "  Downloading pyglet-2.0.0-py3-none-any.whl (966 kB)\n",
            "\u001b[K     |████████████████████████████████| 966 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pyrender->visualization) (2.6.3)\n",
            "Collecting freetype-py\n",
            "  Downloading freetype_py-2.3.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (978 kB)\n",
            "\u001b[K     |████████████████████████████████| 978 kB 59.4 MB/s \n",
            "\u001b[?25hCollecting PyOpenGL==3.1.0\n",
            "  Downloading PyOpenGL-3.1.0.zip (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 57.6 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->autolab-core->visualization) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->autolab-core->visualization) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autolab-core->visualization) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.0.4)\n",
            "Collecting rtree\n",
            "  Downloading Rtree-1.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (2.23.0)\n",
            "Collecting svg.path\n",
            "  Downloading svg.path-6.2-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: shapely in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.8.5.post1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (4.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (57.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (3.0.4)\n",
            "Collecting mapbox-earcut\n",
            "  Downloading mapbox_earcut-1.0.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 9.5 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 70.1 MB/s \n",
            "\u001b[?25hCollecting pyglet>=1.4.10\n",
            "  Downloading pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 63.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.7.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (4.3.3)\n",
            "Collecting pycollada\n",
            "  Downloading pycollada-0.7.2.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 70.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (5.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (4.13.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (0.19.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (22.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->trimesh[easy]->visualization) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->trimesh[easy]->visualization) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->trimesh[easy]->visualization) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->trimesh[easy]->visualization) (2.10)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->trimesh[easy]->visualization) (1.2.1)\n",
            "Building wheels for collected packages: PyOpenGL, pycollada\n",
            "  Building wheel for PyOpenGL (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyOpenGL: filename=PyOpenGL-3.1.0-py3-none-any.whl size=1745210 sha256=fc256f16b72960ab7393aa5c232686f54d1dba2d5de8ca90c8dabf7e25776805\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/83/cb/af51a0c06c33d08537b941bbfc87469e8a3c68d05f77a6a212\n",
            "  Building wheel for pycollada (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycollada: filename=pycollada-0.7.2-py3-none-any.whl size=127026 sha256=ae4ae63d10a463f17fbf71486b6d93f7c50a64df72500332c85f3b4856c2e608\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/0b/be/6cad4a774484615180dc30a68ff3635fbc36cccf489bcd6543\n",
            "Successfully built PyOpenGL pycollada\n",
            "Installing collected packages: ruamel.yaml.clib, xxhash, trimesh, svg.path, setproctitle, ruamel.yaml, rtree, PyOpenGL, pyglet, pycollada, multiprocess, mapbox-earcut, freetype-py, colorlog, pyrender, autolab-core, visualization\n",
            "  Attempting uninstall: PyOpenGL\n",
            "    Found existing installation: PyOpenGL 3.1.6\n",
            "    Uninstalling PyOpenGL-3.1.6:\n",
            "      Successfully uninstalled PyOpenGL-3.1.6\n",
            "Successfully installed PyOpenGL-3.1.0 autolab-core-1.1.1 colorlog-6.7.0 freetype-py-2.3.0 mapbox-earcut-1.0.0 multiprocess-0.70.14 pycollada-0.7.2 pyglet-1.5.27 pyrender-0.1.45 rtree-1.0.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 setproctitle-1.3.2 svg.path-6.2 trimesh-3.16.4 visualization-1.0.0 xxhash-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.xception import Xception\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Flatten, Dense, Input, Reshape, Lambda\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "#from func_utils import *\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
        "#os.environ['OMP_NUM_THREADS']='6'\n",
        "batch_size = 32\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "EOluVeb9Hekr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "#for i,layer in enumerate(base_model.layers):\n",
        "    #print (i,layer.name)\n",
        "input = Input(shape=(300,300,3),name='input1')\n",
        "input_gender = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output = base_model(input)\n",
        "gender_embedding=Dense(16)(input_gender)\n",
        "#gender_embedding=Dense(12)(gender_embedding)\n",
        "#x = keras.layers.MaxPooling2D(pool_size=(3,3))(output)\n",
        "#x = keras.layers.Conv2D(512,kernel_size=(3,3))(x)\n",
        "#x = keras.layers.Conv2D(256,kernel_size=(1,1))(x)\n",
        "print (K.int_shape(output))\n",
        "x = keras.layers.MaxPooling2D(pool_size=(8,8))(output)\n",
        "print (K.int_shape(x))\n",
        "x=Flatten()(x)\n",
        "f = keras.layers.Concatenate(axis=1)([x,gender_embedding])\n",
        "print (K.int_shape(f)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "predictions = Dense(240)(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC1IsC1iIIb_",
        "outputId": "284923cd-d0ac-4893-edc9-cc033384fe83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 10, 10, 2048)\n",
            "(None, 1, 1, 2048)\n",
            "(None, 2064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input,input_gender], outputs=predictions)\n",
        "for i,layer in enumerate(model.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0001,beta_1=0.9,beta_2=0.999)\n",
        "model.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])\n",
        "\n",
        "# Save weights after every epoch\n",
        "#dr='/content/drive/MyDrive/Colab Notebooks/weights'\n",
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "history=model.fit([x_train,gender_train],y_train,batch_size=batch_size,epochs=100,verbose=1,validation_data=([x_valid,gender_valid],y_valid), callbacks = [checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7BFB866IY9O",
        "outputId": "83ef2a00-545e-4cc9-dcc7-214b5d67b418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input1\n",
            "1 resnet50\n",
            "2 max_pooling2d_18\n",
            "3 flatten_6\n",
            "4 input2\n",
            "5 dense_13\n",
            "Epoch 1/100\n",
            "2/2 [==============================] - 8s 1s/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.4090 - val_MAE: 0.4090\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.2014 - MAE: 0.2014 - val_loss: 0.4212 - val_MAE: 0.4212\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.2023 - MAE: 0.2023 - val_loss: 0.4231 - val_MAE: 0.4231\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 1s 449ms/step - loss: 0.2021 - MAE: 0.2021 - val_loss: 0.4175 - val_MAE: 0.4175\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 1s 450ms/step - loss: 0.2019 - MAE: 0.2019 - val_loss: 0.4283 - val_MAE: 0.4283\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 0.4375 - val_MAE: 0.4375\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 1s 450ms/step - loss: 0.2015 - MAE: 0.2015 - val_loss: 0.4357 - val_MAE: 0.4357\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 1s 449ms/step - loss: 0.2017 - MAE: 0.2017 - val_loss: 0.4138 - val_MAE: 0.4138\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.4007 - val_MAE: 0.4007\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 1s 448ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 0.3938 - val_MAE: 0.3938\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 0.3934 - val_MAE: 0.3934\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.3930 - val_MAE: 0.3930\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 0.2015 - MAE: 0.2015 - val_loss: 0.3916 - val_MAE: 0.3916\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3837 - val_MAE: 0.3837\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3702 - val_MAE: 0.3702\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3571 - val_MAE: 0.3571\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.3460 - val_MAE: 0.3460\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3349 - val_MAE: 0.3349\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3232 - val_MAE: 0.3232\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 1s 443ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3114 - val_MAE: 0.3114\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 1s 444ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2990 - val_MAE: 0.2990\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2871 - val_MAE: 0.2871\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2762 - val_MAE: 0.2762\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2662 - val_MAE: 0.2662\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 0.2587 - val_MAE: 0.2587\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 1s 444ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2524 - val_MAE: 0.2524\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2468 - val_MAE: 0.2468\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2412 - val_MAE: 0.2412\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2363 - val_MAE: 0.2363\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 1s 796ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2319 - val_MAE: 0.2319\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 0.2015 - MAE: 0.2015 - val_loss: 0.2274 - val_MAE: 0.2274\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2287 - val_MAE: 0.2287\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2304 - val_MAE: 0.2304\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2307 - val_MAE: 0.2307\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 1s 460ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2307 - val_MAE: 0.2307\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2305 - val_MAE: 0.2305\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2300 - val_MAE: 0.2300\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 1s 475ms/step - loss: 0.2014 - MAE: 0.2014 - val_loss: 0.2299 - val_MAE: 0.2299\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 1s 474ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2298 - val_MAE: 0.2298\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2297 - val_MAE: 0.2297\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2292 - val_MAE: 0.2292\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 1s 477ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2283 - val_MAE: 0.2283\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2265 - val_MAE: 0.2265\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 1s 445ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2230 - val_MAE: 0.2230\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2214 - val_MAE: 0.2214\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2202 - val_MAE: 0.2202\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 1s 442ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 0.2210 - val_MAE: 0.2210\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 0.2215 - val_MAE: 0.2215\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2217 - val_MAE: 0.2217\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 1s 459ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 0.2212 - val_MAE: 0.2212\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2206 - val_MAE: 0.2206\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2200 - val_MAE: 0.2200\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2195 - val_MAE: 0.2195\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2188 - val_MAE: 0.2188\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2181 - val_MAE: 0.2181\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2172 - val_MAE: 0.2172\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2159 - val_MAE: 0.2159\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2145 - val_MAE: 0.2145\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2137 - val_MAE: 0.2137\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 1s 739ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2130 - val_MAE: 0.2130\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2122 - val_MAE: 0.2122\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2115 - val_MAE: 0.2115\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2110 - val_MAE: 0.2110\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2106 - val_MAE: 0.2106\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2103 - val_MAE: 0.2103\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2101 - val_MAE: 0.2101\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 1s 450ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2104 - val_MAE: 0.2104\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2109 - val_MAE: 0.2109\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2117 - val_MAE: 0.2117\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2122 - val_MAE: 0.2122\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2124 - val_MAE: 0.2124\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2126 - val_MAE: 0.2126\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2129 - val_MAE: 0.2129\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2130 - val_MAE: 0.2130\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2129 - val_MAE: 0.2129\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2127 - val_MAE: 0.2127\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2124 - val_MAE: 0.2124\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 1s 451ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2123 - val_MAE: 0.2123\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 1s 440ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2122 - val_MAE: 0.2122\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2121 - val_MAE: 0.2121\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2118 - val_MAE: 0.2118\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2115 - val_MAE: 0.2115\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2111 - val_MAE: 0.2111\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2107 - val_MAE: 0.2107\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2103 - val_MAE: 0.2103\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 1s 433ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2099 - val_MAE: 0.2099\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2095 - val_MAE: 0.2095\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2091 - val_MAE: 0.2091\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2087 - val_MAE: 0.2087\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 1s 760ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2082 - val_MAE: 0.2082\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2078 - val_MAE: 0.2078\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 1s 439ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2074 - val_MAE: 0.2074\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2071 - val_MAE: 0.2071\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 1s 432ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2069 - val_MAE: 0.2069\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 1s 436ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2066 - val_MAE: 0.2066\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 1s 435ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2064 - val_MAE: 0.2064\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 1s 434ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2063 - val_MAE: 0.2063\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 1s 438ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2062 - val_MAE: 0.2062\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2061 - val_MAE: 0.2061\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 1s 431ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2057 - val_MAE: 0.2057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model1 = InceptionV3(weights='imagenet', include_top=False)\n",
        "for i,layer in enumerate(base_model1.layers):\n",
        "    print (i,layer.name)\n",
        "input1 = Input(shape=(300,300,3),name='input1')\n",
        "input_gender1 = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output1 = base_model1(input1)\n",
        "gender_embedding1=Dense(16)(input_gender1)\n",
        "#gender_embedding=Dense(12)(gender_embedding)\n",
        "#x = keras.layers.MaxPooling2D(pool_size=(3,3))(output)\n",
        "#x = keras.layers.Conv2D(512,kernel_size=(3,3))(x)\n",
        "#x = keras.layers.Conv2D(256,kernel_size=(1,1))(x)\n",
        "print (K.int_shape(output1))\n",
        "x1 = keras.layers.MaxPooling2D(pool_size=(8,8))(output1)\n",
        "print (K.int_shape(x))\n",
        "x1=Flatten()(x1)\n",
        "f = keras.layers.Concatenate(axis=1)([x1,gender_embedding1])\n",
        "print (K.int_shape(f)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "prediction1 = Dense(240)(x1)\n",
        "\n",
        "model1 = Model(inputs=[input1,input_gender1], outputs=prediction1)\n",
        "for i,layer in enumerate(model1.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0003,beta_1=0.9,beta_2=0.999)\n",
        "model1.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])\n",
        "\n",
        "# Save weights after every epoch\n",
        "#dr='/content/drive/MyDrive/Colab Notebooks/weights'\n",
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "history1=model1.fit([x_train,gender_train],y_train,batch_size=batch_size,epochs=100,verbose=1,validation_data=([x_valid,gender_valid],y_valid), callbacks = [checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwjVll-0JGjQ",
        "outputId": "ff983ea9-1ede-42b2-a370-ca0634e1da87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_10\n",
            "1 conv2d_470\n",
            "2 batch_normalization_470\n",
            "3 activation_470\n",
            "4 conv2d_471\n",
            "5 batch_normalization_471\n",
            "6 activation_471\n",
            "7 conv2d_472\n",
            "8 batch_normalization_472\n",
            "9 activation_472\n",
            "10 max_pooling2d_29\n",
            "11 conv2d_473\n",
            "12 batch_normalization_473\n",
            "13 activation_473\n",
            "14 conv2d_474\n",
            "15 batch_normalization_474\n",
            "16 activation_474\n",
            "17 max_pooling2d_30\n",
            "18 conv2d_478\n",
            "19 batch_normalization_478\n",
            "20 activation_478\n",
            "21 conv2d_476\n",
            "22 conv2d_479\n",
            "23 batch_normalization_476\n",
            "24 batch_normalization_479\n",
            "25 activation_476\n",
            "26 activation_479\n",
            "27 average_pooling2d_45\n",
            "28 conv2d_475\n",
            "29 conv2d_477\n",
            "30 conv2d_480\n",
            "31 conv2d_481\n",
            "32 batch_normalization_475\n",
            "33 batch_normalization_477\n",
            "34 batch_normalization_480\n",
            "35 batch_normalization_481\n",
            "36 activation_475\n",
            "37 activation_477\n",
            "38 activation_480\n",
            "39 activation_481\n",
            "40 mixed0\n",
            "41 conv2d_485\n",
            "42 batch_normalization_485\n",
            "43 activation_485\n",
            "44 conv2d_483\n",
            "45 conv2d_486\n",
            "46 batch_normalization_483\n",
            "47 batch_normalization_486\n",
            "48 activation_483\n",
            "49 activation_486\n",
            "50 average_pooling2d_46\n",
            "51 conv2d_482\n",
            "52 conv2d_484\n",
            "53 conv2d_487\n",
            "54 conv2d_488\n",
            "55 batch_normalization_482\n",
            "56 batch_normalization_484\n",
            "57 batch_normalization_487\n",
            "58 batch_normalization_488\n",
            "59 activation_482\n",
            "60 activation_484\n",
            "61 activation_487\n",
            "62 activation_488\n",
            "63 mixed1\n",
            "64 conv2d_492\n",
            "65 batch_normalization_492\n",
            "66 activation_492\n",
            "67 conv2d_490\n",
            "68 conv2d_493\n",
            "69 batch_normalization_490\n",
            "70 batch_normalization_493\n",
            "71 activation_490\n",
            "72 activation_493\n",
            "73 average_pooling2d_47\n",
            "74 conv2d_489\n",
            "75 conv2d_491\n",
            "76 conv2d_494\n",
            "77 conv2d_495\n",
            "78 batch_normalization_489\n",
            "79 batch_normalization_491\n",
            "80 batch_normalization_494\n",
            "81 batch_normalization_495\n",
            "82 activation_489\n",
            "83 activation_491\n",
            "84 activation_494\n",
            "85 activation_495\n",
            "86 mixed2\n",
            "87 conv2d_497\n",
            "88 batch_normalization_497\n",
            "89 activation_497\n",
            "90 conv2d_498\n",
            "91 batch_normalization_498\n",
            "92 activation_498\n",
            "93 conv2d_496\n",
            "94 conv2d_499\n",
            "95 batch_normalization_496\n",
            "96 batch_normalization_499\n",
            "97 activation_496\n",
            "98 activation_499\n",
            "99 max_pooling2d_31\n",
            "100 mixed3\n",
            "101 conv2d_504\n",
            "102 batch_normalization_504\n",
            "103 activation_504\n",
            "104 conv2d_505\n",
            "105 batch_normalization_505\n",
            "106 activation_505\n",
            "107 conv2d_501\n",
            "108 conv2d_506\n",
            "109 batch_normalization_501\n",
            "110 batch_normalization_506\n",
            "111 activation_501\n",
            "112 activation_506\n",
            "113 conv2d_502\n",
            "114 conv2d_507\n",
            "115 batch_normalization_502\n",
            "116 batch_normalization_507\n",
            "117 activation_502\n",
            "118 activation_507\n",
            "119 average_pooling2d_48\n",
            "120 conv2d_500\n",
            "121 conv2d_503\n",
            "122 conv2d_508\n",
            "123 conv2d_509\n",
            "124 batch_normalization_500\n",
            "125 batch_normalization_503\n",
            "126 batch_normalization_508\n",
            "127 batch_normalization_509\n",
            "128 activation_500\n",
            "129 activation_503\n",
            "130 activation_508\n",
            "131 activation_509\n",
            "132 mixed4\n",
            "133 conv2d_514\n",
            "134 batch_normalization_514\n",
            "135 activation_514\n",
            "136 conv2d_515\n",
            "137 batch_normalization_515\n",
            "138 activation_515\n",
            "139 conv2d_511\n",
            "140 conv2d_516\n",
            "141 batch_normalization_511\n",
            "142 batch_normalization_516\n",
            "143 activation_511\n",
            "144 activation_516\n",
            "145 conv2d_512\n",
            "146 conv2d_517\n",
            "147 batch_normalization_512\n",
            "148 batch_normalization_517\n",
            "149 activation_512\n",
            "150 activation_517\n",
            "151 average_pooling2d_49\n",
            "152 conv2d_510\n",
            "153 conv2d_513\n",
            "154 conv2d_518\n",
            "155 conv2d_519\n",
            "156 batch_normalization_510\n",
            "157 batch_normalization_513\n",
            "158 batch_normalization_518\n",
            "159 batch_normalization_519\n",
            "160 activation_510\n",
            "161 activation_513\n",
            "162 activation_518\n",
            "163 activation_519\n",
            "164 mixed5\n",
            "165 conv2d_524\n",
            "166 batch_normalization_524\n",
            "167 activation_524\n",
            "168 conv2d_525\n",
            "169 batch_normalization_525\n",
            "170 activation_525\n",
            "171 conv2d_521\n",
            "172 conv2d_526\n",
            "173 batch_normalization_521\n",
            "174 batch_normalization_526\n",
            "175 activation_521\n",
            "176 activation_526\n",
            "177 conv2d_522\n",
            "178 conv2d_527\n",
            "179 batch_normalization_522\n",
            "180 batch_normalization_527\n",
            "181 activation_522\n",
            "182 activation_527\n",
            "183 average_pooling2d_50\n",
            "184 conv2d_520\n",
            "185 conv2d_523\n",
            "186 conv2d_528\n",
            "187 conv2d_529\n",
            "188 batch_normalization_520\n",
            "189 batch_normalization_523\n",
            "190 batch_normalization_528\n",
            "191 batch_normalization_529\n",
            "192 activation_520\n",
            "193 activation_523\n",
            "194 activation_528\n",
            "195 activation_529\n",
            "196 mixed6\n",
            "197 conv2d_534\n",
            "198 batch_normalization_534\n",
            "199 activation_534\n",
            "200 conv2d_535\n",
            "201 batch_normalization_535\n",
            "202 activation_535\n",
            "203 conv2d_531\n",
            "204 conv2d_536\n",
            "205 batch_normalization_531\n",
            "206 batch_normalization_536\n",
            "207 activation_531\n",
            "208 activation_536\n",
            "209 conv2d_532\n",
            "210 conv2d_537\n",
            "211 batch_normalization_532\n",
            "212 batch_normalization_537\n",
            "213 activation_532\n",
            "214 activation_537\n",
            "215 average_pooling2d_51\n",
            "216 conv2d_530\n",
            "217 conv2d_533\n",
            "218 conv2d_538\n",
            "219 conv2d_539\n",
            "220 batch_normalization_530\n",
            "221 batch_normalization_533\n",
            "222 batch_normalization_538\n",
            "223 batch_normalization_539\n",
            "224 activation_530\n",
            "225 activation_533\n",
            "226 activation_538\n",
            "227 activation_539\n",
            "228 mixed7\n",
            "229 conv2d_542\n",
            "230 batch_normalization_542\n",
            "231 activation_542\n",
            "232 conv2d_543\n",
            "233 batch_normalization_543\n",
            "234 activation_543\n",
            "235 conv2d_540\n",
            "236 conv2d_544\n",
            "237 batch_normalization_540\n",
            "238 batch_normalization_544\n",
            "239 activation_540\n",
            "240 activation_544\n",
            "241 conv2d_541\n",
            "242 conv2d_545\n",
            "243 batch_normalization_541\n",
            "244 batch_normalization_545\n",
            "245 activation_541\n",
            "246 activation_545\n",
            "247 max_pooling2d_32\n",
            "248 mixed8\n",
            "249 conv2d_550\n",
            "250 batch_normalization_550\n",
            "251 activation_550\n",
            "252 conv2d_547\n",
            "253 conv2d_551\n",
            "254 batch_normalization_547\n",
            "255 batch_normalization_551\n",
            "256 activation_547\n",
            "257 activation_551\n",
            "258 conv2d_548\n",
            "259 conv2d_549\n",
            "260 conv2d_552\n",
            "261 conv2d_553\n",
            "262 average_pooling2d_52\n",
            "263 conv2d_546\n",
            "264 batch_normalization_548\n",
            "265 batch_normalization_549\n",
            "266 batch_normalization_552\n",
            "267 batch_normalization_553\n",
            "268 conv2d_554\n",
            "269 batch_normalization_546\n",
            "270 activation_548\n",
            "271 activation_549\n",
            "272 activation_552\n",
            "273 activation_553\n",
            "274 batch_normalization_554\n",
            "275 activation_546\n",
            "276 mixed9_0\n",
            "277 concatenate_19\n",
            "278 activation_554\n",
            "279 mixed9\n",
            "280 conv2d_559\n",
            "281 batch_normalization_559\n",
            "282 activation_559\n",
            "283 conv2d_556\n",
            "284 conv2d_560\n",
            "285 batch_normalization_556\n",
            "286 batch_normalization_560\n",
            "287 activation_556\n",
            "288 activation_560\n",
            "289 conv2d_557\n",
            "290 conv2d_558\n",
            "291 conv2d_561\n",
            "292 conv2d_562\n",
            "293 average_pooling2d_53\n",
            "294 conv2d_555\n",
            "295 batch_normalization_557\n",
            "296 batch_normalization_558\n",
            "297 batch_normalization_561\n",
            "298 batch_normalization_562\n",
            "299 conv2d_563\n",
            "300 batch_normalization_555\n",
            "301 activation_557\n",
            "302 activation_558\n",
            "303 activation_561\n",
            "304 activation_562\n",
            "305 batch_normalization_563\n",
            "306 activation_555\n",
            "307 mixed9_1\n",
            "308 concatenate_20\n",
            "309 activation_563\n",
            "310 mixed10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 8, 8, 2048)\n",
            "(None, 2048)\n",
            "(None, 2064)\n",
            "0 input1\n",
            "1 inception_v3\n",
            "2 max_pooling2d_33\n",
            "3 flatten_9\n",
            "4 input2\n",
            "5 dense_19\n",
            "Epoch 1/100\n",
            "2/2 [==============================] - 9s 2s/step - loss: 2.1989 - MAE: 2.1989 - val_loss: 4.8638 - val_MAE: 4.8638\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.7951 - MAE: 0.7951 - val_loss: 4.2771 - val_MAE: 4.2771\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.4595 - MAE: 0.4595 - val_loss: 5.5066 - val_MAE: 5.5066\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 1s 341ms/step - loss: 0.3643 - MAE: 0.3643 - val_loss: 7.3969 - val_MAE: 7.3969\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.2996 - MAE: 0.2996 - val_loss: 8.2591 - val_MAE: 8.2591\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 1s 348ms/step - loss: 0.2765 - MAE: 0.2765 - val_loss: 7.5952 - val_MAE: 7.5952\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.2522 - MAE: 0.2522 - val_loss: 5.9420 - val_MAE: 5.9420\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.2453 - MAE: 0.2453 - val_loss: 5.0532 - val_MAE: 5.0532\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 1s 351ms/step - loss: 0.2362 - MAE: 0.2362 - val_loss: 4.0609 - val_MAE: 4.0609\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.2425 - MAE: 0.2425 - val_loss: 3.4931 - val_MAE: 3.4931\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 1s 342ms/step - loss: 0.2304 - MAE: 0.2304 - val_loss: 3.0028 - val_MAE: 3.0028\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.2266 - MAE: 0.2266 - val_loss: 2.6258 - val_MAE: 2.6258\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 1s 341ms/step - loss: 0.2222 - MAE: 0.2222 - val_loss: 2.2788 - val_MAE: 2.2788\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.2169 - MAE: 0.2169 - val_loss: 1.8594 - val_MAE: 1.8594\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.2160 - MAE: 0.2160 - val_loss: 1.5604 - val_MAE: 1.5604\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 1s 346ms/step - loss: 0.2150 - MAE: 0.2150 - val_loss: 1.2670 - val_MAE: 1.2670\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.2080 - MAE: 0.2080 - val_loss: 1.0025 - val_MAE: 1.0025\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.2072 - MAE: 0.2072 - val_loss: 0.7704 - val_MAE: 0.7704\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 0.2057 - MAE: 0.2057 - val_loss: 0.6056 - val_MAE: 0.6056\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.2060 - MAE: 0.2060 - val_loss: 0.4743 - val_MAE: 0.4743\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 0.2025 - MAE: 0.2025 - val_loss: 0.3894 - val_MAE: 0.3894\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.3369 - val_MAE: 0.3369\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 0.1991 - MAE: 0.1991 - val_loss: 0.3071 - val_MAE: 0.3071\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 0.1963 - MAE: 0.1963 - val_loss: 0.2878 - val_MAE: 0.2878\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.1946 - MAE: 0.1946 - val_loss: 0.2725 - val_MAE: 0.2725\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 0.1967 - MAE: 0.1967 - val_loss: 0.2632 - val_MAE: 0.2632\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 1s 349ms/step - loss: 0.1978 - MAE: 0.1978 - val_loss: 0.2505 - val_MAE: 0.2505\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 0.1926 - MAE: 0.1926 - val_loss: 0.2440 - val_MAE: 0.2440\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 1s 352ms/step - loss: 0.1894 - MAE: 0.1894 - val_loss: 0.2413 - val_MAE: 0.2413\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 1s 755ms/step - loss: 0.1880 - MAE: 0.1880 - val_loss: 0.2384 - val_MAE: 0.2384\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.1819 - MAE: 0.1819 - val_loss: 0.2367 - val_MAE: 0.2367\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 1s 348ms/step - loss: 0.1773 - MAE: 0.1773 - val_loss: 0.2359 - val_MAE: 0.2359\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 1s 351ms/step - loss: 0.1742 - MAE: 0.1742 - val_loss: 0.2355 - val_MAE: 0.2355\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 1s 348ms/step - loss: 0.1695 - MAE: 0.1695 - val_loss: 0.2351 - val_MAE: 0.2351\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 1s 364ms/step - loss: 0.1624 - MAE: 0.1624 - val_loss: 0.2345 - val_MAE: 0.2345\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 1s 374ms/step - loss: 0.1561 - MAE: 0.1561 - val_loss: 0.2348 - val_MAE: 0.2348\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 1s 367ms/step - loss: 0.1511 - MAE: 0.1511 - val_loss: 0.2357 - val_MAE: 0.2357\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 1s 371ms/step - loss: 0.1493 - MAE: 0.1493 - val_loss: 0.2375 - val_MAE: 0.2375\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 1s 377ms/step - loss: 0.1466 - MAE: 0.1466 - val_loss: 0.2378 - val_MAE: 0.2378\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 0.1334 - MAE: 0.1334 - val_loss: 0.2393 - val_MAE: 0.2393\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 0.1293 - MAE: 0.1293 - val_loss: 0.2406 - val_MAE: 0.2406\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 1s 348ms/step - loss: 0.1234 - MAE: 0.1234 - val_loss: 0.2413 - val_MAE: 0.2413\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 1s 348ms/step - loss: 0.1217 - MAE: 0.1217 - val_loss: 0.2394 - val_MAE: 0.2394\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 1s 349ms/step - loss: 0.1188 - MAE: 0.1188 - val_loss: 0.2369 - val_MAE: 0.2369\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 1s 346ms/step - loss: 0.1109 - MAE: 0.1109 - val_loss: 0.2359 - val_MAE: 0.2359\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.1071 - MAE: 0.1071 - val_loss: 0.2334 - val_MAE: 0.2334\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.1020 - MAE: 0.1020 - val_loss: 0.2318 - val_MAE: 0.2318\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 1s 350ms/step - loss: 0.0998 - MAE: 0.0998 - val_loss: 0.2300 - val_MAE: 0.2300\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 1s 365ms/step - loss: 0.0991 - MAE: 0.0991 - val_loss: 0.2308 - val_MAE: 0.2308\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.0909 - MAE: 0.0909 - val_loss: 0.2333 - val_MAE: 0.2333\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 0.0888 - MAE: 0.0888 - val_loss: 0.2377 - val_MAE: 0.2377\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 1s 365ms/step - loss: 0.0830 - MAE: 0.0830 - val_loss: 0.2354 - val_MAE: 0.2354\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 1s 348ms/step - loss: 0.0772 - MAE: 0.0772 - val_loss: 0.2331 - val_MAE: 0.2331\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.0752 - MAE: 0.0752 - val_loss: 0.2320 - val_MAE: 0.2320\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.0755 - MAE: 0.0755 - val_loss: 0.2314 - val_MAE: 0.2314\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.0782 - MAE: 0.0782 - val_loss: 0.2296 - val_MAE: 0.2296\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 1s 346ms/step - loss: 0.0677 - MAE: 0.0677 - val_loss: 0.2308 - val_MAE: 0.2308\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.0638 - MAE: 0.0638 - val_loss: 0.2292 - val_MAE: 0.2292\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 0.0664 - MAE: 0.0664 - val_loss: 0.2314 - val_MAE: 0.2314\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 1s 680ms/step - loss: 0.0631 - MAE: 0.0631 - val_loss: 0.2359 - val_MAE: 0.2359\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 1s 341ms/step - loss: 0.0592 - MAE: 0.0592 - val_loss: 0.2331 - val_MAE: 0.2331\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 0.0563 - MAE: 0.0563 - val_loss: 0.2319 - val_MAE: 0.2319\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 0.0584 - MAE: 0.0584 - val_loss: 0.2298 - val_MAE: 0.2298\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 0.0566 - MAE: 0.0566 - val_loss: 0.2308 - val_MAE: 0.2308\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 1s 348ms/step - loss: 0.0514 - MAE: 0.0514 - val_loss: 0.2281 - val_MAE: 0.2281\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 1s 350ms/step - loss: 0.0535 - MAE: 0.0535 - val_loss: 0.2286 - val_MAE: 0.2286\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.0480 - MAE: 0.0480 - val_loss: 0.2271 - val_MAE: 0.2271\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 1s 353ms/step - loss: 0.0514 - MAE: 0.0514 - val_loss: 0.2233 - val_MAE: 0.2233\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.0566 - MAE: 0.0566 - val_loss: 0.2208 - val_MAE: 0.2208\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.0543 - MAE: 0.0543 - val_loss: 0.2232 - val_MAE: 0.2232\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.0462 - MAE: 0.0462 - val_loss: 0.2243 - val_MAE: 0.2243\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 1s 361ms/step - loss: 0.0569 - MAE: 0.0569 - val_loss: 0.2254 - val_MAE: 0.2254\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 1s 346ms/step - loss: 0.0461 - MAE: 0.0461 - val_loss: 0.2330 - val_MAE: 0.2330\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.0429 - MAE: 0.0429 - val_loss: 0.2332 - val_MAE: 0.2332\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 0.0497 - MAE: 0.0497 - val_loss: 0.2271 - val_MAE: 0.2271\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.0492 - MAE: 0.0492 - val_loss: 0.2193 - val_MAE: 0.2193\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 1s 341ms/step - loss: 0.0490 - MAE: 0.0490 - val_loss: 0.2177 - val_MAE: 0.2177\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 0.0442 - MAE: 0.0442 - val_loss: 0.2222 - val_MAE: 0.2222\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.0425 - MAE: 0.0425 - val_loss: 0.2172 - val_MAE: 0.2172\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.0531 - MAE: 0.0531 - val_loss: 0.2165 - val_MAE: 0.2165\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 1s 342ms/step - loss: 0.0408 - MAE: 0.0408 - val_loss: 0.2226 - val_MAE: 0.2226\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 1s 342ms/step - loss: 0.0464 - MAE: 0.0464 - val_loss: 0.2224 - val_MAE: 0.2224\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.0444 - MAE: 0.0444 - val_loss: 0.2185 - val_MAE: 0.2185\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 0.0413 - MAE: 0.0413 - val_loss: 0.2151 - val_MAE: 0.2151\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 1s 341ms/step - loss: 0.0512 - MAE: 0.0512 - val_loss: 0.2165 - val_MAE: 0.2165\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.0376 - MAE: 0.0376 - val_loss: 0.2198 - val_MAE: 0.2198\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 1s 342ms/step - loss: 0.0413 - MAE: 0.0413 - val_loss: 0.2177 - val_MAE: 0.2177\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 1s 345ms/step - loss: 0.0426 - MAE: 0.0426 - val_loss: 0.2155 - val_MAE: 0.2155\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.0417 - MAE: 0.0417 - val_loss: 0.2156 - val_MAE: 0.2156\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 1s 671ms/step - loss: 0.0409 - MAE: 0.0409 - val_loss: 0.2193 - val_MAE: 0.2193\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 1s 343ms/step - loss: 0.0388 - MAE: 0.0388 - val_loss: 0.2167 - val_MAE: 0.2167\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 1s 342ms/step - loss: 0.0392 - MAE: 0.0392 - val_loss: 0.2142 - val_MAE: 0.2142\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 1s 342ms/step - loss: 0.0516 - MAE: 0.0516 - val_loss: 0.2115 - val_MAE: 0.2115\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 1s 342ms/step - loss: 0.0421 - MAE: 0.0421 - val_loss: 0.2113 - val_MAE: 0.2113\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 1s 340ms/step - loss: 0.0441 - MAE: 0.0441 - val_loss: 0.2092 - val_MAE: 0.2092\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 1s 341ms/step - loss: 0.0380 - MAE: 0.0380 - val_loss: 0.2100 - val_MAE: 0.2100\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 1s 348ms/step - loss: 0.0447 - MAE: 0.0447 - val_loss: 0.2119 - val_MAE: 0.2119\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 1s 344ms/step - loss: 0.0420 - MAE: 0.0420 - val_loss: 0.2077 - val_MAE: 0.2077\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 1s 347ms/step - loss: 0.0368 - MAE: 0.0368 - val_loss: 0.2070 - val_MAE: 0.2070\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 1s 346ms/step - loss: 0.0464 - MAE: 0.0464 - val_loss: 0.2071 - val_MAE: 0.2071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "-0jP4atXKnEu",
        "outputId": "6dae1dc7-a06e-4f68-c6b8-909a7d21e47a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history1.history['loss']\n",
        "val_loss = history1.history['val_loss']\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "iIe6CyJtZXiC",
        "outputId": "abdcb6b4-a7e5-45ad-91da-0ed7d49619e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model11 = ResNet50(weights='imagenet', include_top=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhhhg5nAZdUq",
        "outputId": "456b4169-a29a-4d8f-d0ae-760a5399619d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model11.trainable = True"
      ],
      "metadata": {
        "id": "fRiHBqmiuoC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of layers in the base model: \", len(base_model11.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model11.layers[:fine_tune_at]:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkVID-8ov2cm",
        "outputId": "38adca20-636b-4985-aa53-d3cc978019e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in the base model:  175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,layer in enumerate(base_model11.layers):\n",
        "    print (i,layer.name)\n",
        "input11 = Input(shape=(300,300,3),name='input1')\n",
        "input_gender11 = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output11 = base_model11(input11)\n",
        "gender_embedding11=Dense(16)(input_gender11)\n",
        "#gender_embedding=Dense(12)(gender_embedding)\n",
        "#x = keras.layers.MaxPooling2D(pool_size=(3,3))(output)\n",
        "#x = keras.layers.Conv2D(512,kernel_size=(3,3))(x)\n",
        "#x = keras.layers.Conv2D(256,kernel_size=(1,1))(x)\n",
        "print (K.int_shape(output11))\n",
        "x11 = keras.layers.MaxPooling2D(pool_size=(8,8))(output11)\n",
        "print (K.int_shape(x11))\n",
        "x11=Flatten()(x11)\n",
        "f11 = keras.layers.Concatenate(axis=1)([x11,gender_embedding11])\n",
        "print (K.int_shape(f11)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "prediction11 = Dense(240)(x11)\n",
        "\n",
        "model11 = Model(inputs=[input11,input_gender11], outputs=prediction11)\n",
        "for i,layer in enumerate(model11.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0003,beta_1=0.9,beta_2=0.999)\n",
        "model11.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I42nwxRjv8WF",
        "outputId": "ef7097ea-2519-4887-e14e-9945dbc0db1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_1\n",
            "1 conv1_pad\n",
            "2 conv1_conv\n",
            "3 conv1_bn\n",
            "4 conv1_relu\n",
            "5 pool1_pad\n",
            "6 pool1_pool\n",
            "7 conv2_block1_1_conv\n",
            "8 conv2_block1_1_bn\n",
            "9 conv2_block1_1_relu\n",
            "10 conv2_block1_2_conv\n",
            "11 conv2_block1_2_bn\n",
            "12 conv2_block1_2_relu\n",
            "13 conv2_block1_0_conv\n",
            "14 conv2_block1_3_conv\n",
            "15 conv2_block1_0_bn\n",
            "16 conv2_block1_3_bn\n",
            "17 conv2_block1_add\n",
            "18 conv2_block1_out\n",
            "19 conv2_block2_1_conv\n",
            "20 conv2_block2_1_bn\n",
            "21 conv2_block2_1_relu\n",
            "22 conv2_block2_2_conv\n",
            "23 conv2_block2_2_bn\n",
            "24 conv2_block2_2_relu\n",
            "25 conv2_block2_3_conv\n",
            "26 conv2_block2_3_bn\n",
            "27 conv2_block2_add\n",
            "28 conv2_block2_out\n",
            "29 conv2_block3_1_conv\n",
            "30 conv2_block3_1_bn\n",
            "31 conv2_block3_1_relu\n",
            "32 conv2_block3_2_conv\n",
            "33 conv2_block3_2_bn\n",
            "34 conv2_block3_2_relu\n",
            "35 conv2_block3_3_conv\n",
            "36 conv2_block3_3_bn\n",
            "37 conv2_block3_add\n",
            "38 conv2_block3_out\n",
            "39 conv3_block1_1_conv\n",
            "40 conv3_block1_1_bn\n",
            "41 conv3_block1_1_relu\n",
            "42 conv3_block1_2_conv\n",
            "43 conv3_block1_2_bn\n",
            "44 conv3_block1_2_relu\n",
            "45 conv3_block1_0_conv\n",
            "46 conv3_block1_3_conv\n",
            "47 conv3_block1_0_bn\n",
            "48 conv3_block1_3_bn\n",
            "49 conv3_block1_add\n",
            "50 conv3_block1_out\n",
            "51 conv3_block2_1_conv\n",
            "52 conv3_block2_1_bn\n",
            "53 conv3_block2_1_relu\n",
            "54 conv3_block2_2_conv\n",
            "55 conv3_block2_2_bn\n",
            "56 conv3_block2_2_relu\n",
            "57 conv3_block2_3_conv\n",
            "58 conv3_block2_3_bn\n",
            "59 conv3_block2_add\n",
            "60 conv3_block2_out\n",
            "61 conv3_block3_1_conv\n",
            "62 conv3_block3_1_bn\n",
            "63 conv3_block3_1_relu\n",
            "64 conv3_block3_2_conv\n",
            "65 conv3_block3_2_bn\n",
            "66 conv3_block3_2_relu\n",
            "67 conv3_block3_3_conv\n",
            "68 conv3_block3_3_bn\n",
            "69 conv3_block3_add\n",
            "70 conv3_block3_out\n",
            "71 conv3_block4_1_conv\n",
            "72 conv3_block4_1_bn\n",
            "73 conv3_block4_1_relu\n",
            "74 conv3_block4_2_conv\n",
            "75 conv3_block4_2_bn\n",
            "76 conv3_block4_2_relu\n",
            "77 conv3_block4_3_conv\n",
            "78 conv3_block4_3_bn\n",
            "79 conv3_block4_add\n",
            "80 conv3_block4_out\n",
            "81 conv4_block1_1_conv\n",
            "82 conv4_block1_1_bn\n",
            "83 conv4_block1_1_relu\n",
            "84 conv4_block1_2_conv\n",
            "85 conv4_block1_2_bn\n",
            "86 conv4_block1_2_relu\n",
            "87 conv4_block1_0_conv\n",
            "88 conv4_block1_3_conv\n",
            "89 conv4_block1_0_bn\n",
            "90 conv4_block1_3_bn\n",
            "91 conv4_block1_add\n",
            "92 conv4_block1_out\n",
            "93 conv4_block2_1_conv\n",
            "94 conv4_block2_1_bn\n",
            "95 conv4_block2_1_relu\n",
            "96 conv4_block2_2_conv\n",
            "97 conv4_block2_2_bn\n",
            "98 conv4_block2_2_relu\n",
            "99 conv4_block2_3_conv\n",
            "100 conv4_block2_3_bn\n",
            "101 conv4_block2_add\n",
            "102 conv4_block2_out\n",
            "103 conv4_block3_1_conv\n",
            "104 conv4_block3_1_bn\n",
            "105 conv4_block3_1_relu\n",
            "106 conv4_block3_2_conv\n",
            "107 conv4_block3_2_bn\n",
            "108 conv4_block3_2_relu\n",
            "109 conv4_block3_3_conv\n",
            "110 conv4_block3_3_bn\n",
            "111 conv4_block3_add\n",
            "112 conv4_block3_out\n",
            "113 conv4_block4_1_conv\n",
            "114 conv4_block4_1_bn\n",
            "115 conv4_block4_1_relu\n",
            "116 conv4_block4_2_conv\n",
            "117 conv4_block4_2_bn\n",
            "118 conv4_block4_2_relu\n",
            "119 conv4_block4_3_conv\n",
            "120 conv4_block4_3_bn\n",
            "121 conv4_block4_add\n",
            "122 conv4_block4_out\n",
            "123 conv4_block5_1_conv\n",
            "124 conv4_block5_1_bn\n",
            "125 conv4_block5_1_relu\n",
            "126 conv4_block5_2_conv\n",
            "127 conv4_block5_2_bn\n",
            "128 conv4_block5_2_relu\n",
            "129 conv4_block5_3_conv\n",
            "130 conv4_block5_3_bn\n",
            "131 conv4_block5_add\n",
            "132 conv4_block5_out\n",
            "133 conv4_block6_1_conv\n",
            "134 conv4_block6_1_bn\n",
            "135 conv4_block6_1_relu\n",
            "136 conv4_block6_2_conv\n",
            "137 conv4_block6_2_bn\n",
            "138 conv4_block6_2_relu\n",
            "139 conv4_block6_3_conv\n",
            "140 conv4_block6_3_bn\n",
            "141 conv4_block6_add\n",
            "142 conv4_block6_out\n",
            "143 conv5_block1_1_conv\n",
            "144 conv5_block1_1_bn\n",
            "145 conv5_block1_1_relu\n",
            "146 conv5_block1_2_conv\n",
            "147 conv5_block1_2_bn\n",
            "148 conv5_block1_2_relu\n",
            "149 conv5_block1_0_conv\n",
            "150 conv5_block1_3_conv\n",
            "151 conv5_block1_0_bn\n",
            "152 conv5_block1_3_bn\n",
            "153 conv5_block1_add\n",
            "154 conv5_block1_out\n",
            "155 conv5_block2_1_conv\n",
            "156 conv5_block2_1_bn\n",
            "157 conv5_block2_1_relu\n",
            "158 conv5_block2_2_conv\n",
            "159 conv5_block2_2_bn\n",
            "160 conv5_block2_2_relu\n",
            "161 conv5_block2_3_conv\n",
            "162 conv5_block2_3_bn\n",
            "163 conv5_block2_add\n",
            "164 conv5_block2_out\n",
            "165 conv5_block3_1_conv\n",
            "166 conv5_block3_1_bn\n",
            "167 conv5_block3_1_relu\n",
            "168 conv5_block3_2_conv\n",
            "169 conv5_block3_2_bn\n",
            "170 conv5_block3_2_relu\n",
            "171 conv5_block3_3_conv\n",
            "172 conv5_block3_3_bn\n",
            "173 conv5_block3_add\n",
            "174 conv5_block3_out\n",
            "(None, 10, 10, 2048)\n",
            "(None, 1, 1, 2048)\n",
            "(None, 2064)\n",
            "0 input1\n",
            "1 resnet50\n",
            "2 max_pooling2d\n",
            "3 flatten\n",
            "4 input2\n",
            "5 dense_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model11.trainable_variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS1a-Sc4w4Ek",
        "outputId": "5c88b157-b055-4e1d-d9e2-b6cc1d20371a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "history11=model11.fit([x_train,gender_train],y_train,batch_size=batch_size,epochs=100,verbose=1,validation_data=([x_valid,gender_valid],y_valid), callbacks = [checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzKVIdnFxAeI",
        "outputId": "b4383034-de85-4f52-ad0c-e96f89108cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 19s 4s/step - loss: 6.0222 - MAE: 6.0222 - val_loss: 2.6998 - val_MAE: 2.6998\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 2.4443 - MAE: 2.4443 - val_loss: 2.0727 - val_MAE: 2.0727\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 1.3692 - MAE: 1.3692 - val_loss: 1.7680 - val_MAE: 1.7680\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.8236 - MAE: 0.8236 - val_loss: 1.8187 - val_MAE: 1.8187\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.5085 - MAE: 0.5085 - val_loss: 2.1730 - val_MAE: 2.1730\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.3515 - MAE: 0.3515 - val_loss: 2.9238 - val_MAE: 2.9238\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2758 - MAE: 0.2758 - val_loss: 4.1629 - val_MAE: 4.1629\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.2403 - MAE: 0.2403 - val_loss: 5.7754 - val_MAE: 5.7754\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2205 - MAE: 0.2205 - val_loss: 7.3289 - val_MAE: 7.3289\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.2173 - MAE: 0.2173 - val_loss: 8.7887 - val_MAE: 8.7887\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.2050 - MAE: 0.2050 - val_loss: 9.3619 - val_MAE: 9.3619\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.2027 - MAE: 0.2027 - val_loss: 9.5306 - val_MAE: 9.5306\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.2018 - MAE: 0.2018 - val_loss: 9.3608 - val_MAE: 9.3608\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.2127 - MAE: 0.2127 - val_loss: 9.4076 - val_MAE: 9.4076\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.2016 - MAE: 0.2016 - val_loss: 8.8720 - val_MAE: 8.8720\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.2017 - MAE: 0.2017 - val_loss: 8.3622 - val_MAE: 8.3622\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.2015 - MAE: 0.2015 - val_loss: 7.7071 - val_MAE: 7.7071\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2014 - MAE: 0.2014 - val_loss: 7.0690 - val_MAE: 7.0690\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 6.4367 - val_MAE: 6.4367\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 5.8180 - val_MAE: 5.8180\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 5.2296 - val_MAE: 5.2296\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 4.6799 - val_MAE: 4.6799\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 4.1868 - val_MAE: 4.1868\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 3.7499 - val_MAE: 3.7499\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 3.3772 - val_MAE: 3.3772\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 3.0475 - val_MAE: 3.0475\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 2.7418 - val_MAE: 2.7418\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 2.4748 - val_MAE: 2.4748\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 2.2404 - val_MAE: 2.2404\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 1s 603ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 2.0347 - val_MAE: 2.0347\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 1.8549 - val_MAE: 1.8549\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 1.6946 - val_MAE: 1.6946\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.5529 - val_MAE: 1.5529\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.4228 - val_MAE: 1.4228\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.3067 - val_MAE: 1.3067\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.2016 - val_MAE: 1.2016\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.1063 - val_MAE: 1.1063\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.0207 - val_MAE: 1.0207\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.9440 - val_MAE: 0.9440\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.8737 - val_MAE: 0.8737\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.8109 - val_MAE: 0.8109\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.7535 - val_MAE: 0.7535\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.7011 - val_MAE: 0.7011\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.6533 - val_MAE: 0.6533\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 1s 284ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.6104 - val_MAE: 0.6104\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 1s 400ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.5715 - val_MAE: 0.5715\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.5364 - val_MAE: 0.5364\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.2014 - MAE: 0.2014 - val_loss: 0.5055 - val_MAE: 0.5055\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.4782 - val_MAE: 0.4782\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.4542 - val_MAE: 0.4542\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.4326 - val_MAE: 0.4326\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.4124 - val_MAE: 0.4124\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3941 - val_MAE: 0.3941\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3776 - val_MAE: 0.3776\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3626 - val_MAE: 0.3626\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3491 - val_MAE: 0.3491\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3370 - val_MAE: 0.3370\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3264 - val_MAE: 0.3264\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3165 - val_MAE: 0.3165\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 1s 587ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3074 - val_MAE: 0.3074\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2990 - val_MAE: 0.2990\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2914 - val_MAE: 0.2914\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2841 - val_MAE: 0.2841\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2775 - val_MAE: 0.2775\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2713 - val_MAE: 0.2713\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2656 - val_MAE: 0.2656\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2602 - val_MAE: 0.2602\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2551 - val_MAE: 0.2551\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2507 - val_MAE: 0.2507\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2463 - val_MAE: 0.2463\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2423 - val_MAE: 0.2423\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2386 - val_MAE: 0.2386\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2349 - val_MAE: 0.2349\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2316 - val_MAE: 0.2316\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2284 - val_MAE: 0.2284\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2254 - val_MAE: 0.2254\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 1s 369ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2228 - val_MAE: 0.2228\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 1s 430ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2205 - val_MAE: 0.2205\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 1s 386ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2184 - val_MAE: 0.2184\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 1s 449ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2166 - val_MAE: 0.2166\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 1s 297ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2151 - val_MAE: 0.2151\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2140 - val_MAE: 0.2140\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 1s 283ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2129 - val_MAE: 0.2129\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2118 - val_MAE: 0.2118\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 1s 286ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2109 - val_MAE: 0.2109\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2099 - val_MAE: 0.2099\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2091 - val_MAE: 0.2091\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2083 - val_MAE: 0.2083\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2077 - val_MAE: 0.2077\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 1s 618ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2071 - val_MAE: 0.2071\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2066 - val_MAE: 0.2066\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 1s 282ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2061 - val_MAE: 0.2061\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2059 - val_MAE: 0.2059\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 1s 283ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2058 - val_MAE: 0.2058\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2057 - val_MAE: 0.2057\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2057 - val_MAE: 0.2057\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2057 - val_MAE: 0.2057\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2057 - val_MAE: 0.2057\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 1s 282ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2057 - val_MAE: 0.2057\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2057 - val_MAE: 0.2057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history11.history['loss']\n",
        "val_loss = history11.history['val_loss']\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "UgSX91P9xQMY",
        "outputId": "cddff422-30ef-4ad9-bf16-c59fc27213c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model12 = ResNet50(weights='imagenet', include_top=False)\n",
        "base_model12.trainable = True\n",
        "print(\"Number of layers in the base model: \", len(base_model12.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model12.layers[:fine_tune_at]:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4dzuYPixqnS",
        "outputId": "40d1f89c-a78a-4083-e8f6-7c023e0ebfac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in the base model:  175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "attention_module = 'cbam_block'\n",
        "model_type = base_model12 if attention_module==None else attention_module"
      ],
      "metadata": {
        "id": "lL6WncAB0hBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models import resnet_v1, resnet_v2, mobilenets, inception_v3, inception_resnet_v2\n",
        "\n",
        "depth = 20 # For ResNet, specify the depth (e.g. ResNet50: depth=50)\n",
        "model = resnet_v1.resnet_v1(input_shape=input_shape, depth=depth, attention_module=attention_module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jbl_aeJv0vwj",
        "outputId": "242e2eba-758b-4cb8-e1c4-f6d698e23861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-850976d2298c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet_v1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmobilenets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minception_v3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minception_resnet_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;31m# For ResNet, specify the depth (e.g. ResNet50: depth=50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Dh9-TNf3a_i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2irLquk+ZBsWqcQgsb+Vs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bone-Age-Maisha/paper_1/blob/main/unfreez_cbam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rzSUVbG67dF",
        "outputId": "be796058-7c98-492a-b496-84da0946c7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_PcIoKKIAJAY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj0M8rgXC9-M",
        "outputId": "1994bb47-8fb1-402e-9210-835e3047d648"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "jge23qZcHDjY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_dir = '/content/drive/MyDrive/small_data/train'\n",
        "df = pd.read_csv('/content/drive/MyDrive/small_data/train_csv1.csv')"
      ],
      "metadata": {
        "id": "Wi5JWkMZHJXf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "tf.config.experimental.set_memory_growth(gpus[0], True)"
      ],
      "metadata": {
        "id": "3JzBDYz5HMbn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_age = []\n",
        "y_gender = []\n",
        "\n",
        "#df = pd.read_csv('/raid/chenchao/code/BoneAge/BoneAge/data/Training.csv')\n",
        "a = df.values\n",
        "m = a.shape[0]\n",
        "\n",
        "path = train_dir\n",
        "k = 0\n",
        "print ('Loading data set...')\n",
        "k=1\n",
        "for i in os.listdir(path):\n",
        "  #print(i)\n",
        "  if(len(i)>9):   #errror occuring  so to \n",
        "    continue\n",
        "  y_age.append(df.boneage[df.id == int(i[:-4])].tolist()[0])\n",
        "  a = df.male[df.id == int(i[:-4])].tolist()[0]\n",
        "  if a:\n",
        "    y_gender.append(1)\n",
        "  else:\n",
        "     y_gender.append(0)\n",
        "  img_path = path + \"/\"+i\n",
        "  img = cv2.imread(img_path)\n",
        "  #print(img.shape)\n",
        "  #print (img_path)\n",
        "  img = cv2.imread(img_path)\n",
        "    #print (img_path)\n",
        "    #if(img is not None):\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img,(300,300))\n",
        "  x = np.asarray(img, dtype=np.uint8)\n",
        "  X_train.append(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNGFFM_CHQLn",
        "outputId": "ef2e24d5-d8d4-4792-a325-c55f55843074"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data set...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softlabel(label,num_class):\n",
        "    softlabel=np.zeros((len(label),num_class))\n",
        "    ratio = 1.0/50\n",
        "    for i in range(len(label)):\n",
        "        for j in range(num_class):\n",
        "            softlabel[i,j]=1.0 - ratio*np.abs(j-label[i])\n",
        "    softlabel = np.maximum(softlabel,0)\n",
        "    return softlabel"
      ],
      "metadata": {
        "id": "Z82u4e5SHYyA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.asarray(y_age)\n",
        "gender = np.asarray(y_gender)\n",
        "x=np.asarray(X_train, dtype=np.float32)\n",
        "x/255\n",
        "gender =2*( gender-0.5)\n",
        "x_final = []\n",
        "y_final = []\n",
        "gender_final = []\n",
        "\n",
        "# Shuffle images and split into train, validation and test sets\n",
        "#random_no = np.random.choice(x.shape[0], size=x.shape[0], replace=False)\n",
        "random_no = np.arange(x.shape[0])\n",
        "#print(random_no)\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(random_no)\n",
        "for i in random_no:\n",
        "    x_final.append(x[i,:,:,:])\n",
        "    y_final.append(y[i])\n",
        "    gender_final.append(gender[i])\n",
        "\n",
        "x_final = np.asarray(x_final)\n",
        "y_final = np.asarray(y_final)\n",
        "gender_final = np.asarray(gender_final)\n",
        "print (y_final[:50])\n",
        "print (gender_final[:50])\n",
        "k = 10 # Decides split count\n",
        "x_test = x_final[:k,:,:,:]\n",
        "y_test = y_final[:k]\n",
        "gender_test = gender_final[:k]\n",
        "x_valid = x_final[k:2*k,:,:,:]\n",
        "y_valid = y_final[k:2*k]\n",
        "gender_valid = gender_final[k:2*k]\n",
        "x_train = x_final[2*k:,:,:,:]\n",
        "y_train = y_final[2*k:]\n",
        "gender_train = gender_final[2*k:]\n",
        "\n",
        "## \n",
        "#y_test = keras.utils.to_categorical(y_test,240)\n",
        "#y_train = keras.utils.to_categorical(y_train,240)\n",
        "#y_valid = keras.utils.to_categorical(y_valid,240)\n",
        "y_train = softlabel(y_train,240)\n",
        "y_valid = softlabel(y_valid,240)\n",
        "y_test = softlabel(y_test,240)\n",
        "print (y_train)\n",
        "\n",
        "\n",
        "print ('x_train shape:'+ str(x_train.shape))\n",
        "print ('y_train shape:'+ str(y_train.shape))\n",
        "print ('gender_train shape:'+ str(gender_train.shape))\n",
        "print ('x_valid shape:'+ str(x_valid.shape))\n",
        "print ('y_valid shape:'+ str(y_valid.shape))\n",
        "print ('gender_valid shape:' + str(gender_valid.shape))\n",
        "print ('x_test shape:'+ str(x_test.shape))\n",
        "print ('y_test shape:'+ str(y_test.shape))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0TN-ZPAH1pI",
        "outputId": "78b66a0e-7e40-4d6b-823c-0b537336d3b1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[126 149 113 132 156 180 156  42 126 126  42  30  78 174  88 165  32 132\n",
            " 156  82 192 170  94  32 156 120  60  33 126  54  27 108  94 162 120  21\n",
            " 188  33 136  24   4  12 132  36  57  24  90 138 138 159]\n",
            "[ 1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
            "  1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1. -1.  1. -1. -1. -1. -1.\n",
            " -1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1.  1.  1. -1.]\n",
            "[[0.   0.   0.   ... 0.1  0.08 0.06]\n",
            " [0.   0.   0.   ... 0.   0.   0.  ]\n",
            " [0.   0.   0.   ... 0.   0.   0.  ]\n",
            " ...\n",
            " [0.   0.02 0.04 ... 0.   0.   0.  ]\n",
            " [0.52 0.54 0.56 ... 0.   0.   0.  ]\n",
            " [0.   0.   0.   ... 0.   0.   0.  ]]\n",
            "x_train shape:(51, 300, 300, 3)\n",
            "y_train shape:(51, 240)\n",
            "gender_train shape:(51,)\n",
            "x_valid shape:(10, 300, 300, 3)\n",
            "y_valid shape:(10, 240)\n",
            "gender_valid shape:(10,)\n",
            "x_test shape:(10, 300, 300, 3)\n",
            "y_test shape:(10, 240)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install visualization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-MKAQxgH6fo",
        "outputId": "db21e79b-94a7-41ba-b227-e8437c602989"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting visualization\n",
            "  Downloading visualization-1.0.0-py3-none-any.whl (13 kB)\n",
            "Collecting pyrender\n",
            "  Downloading pyrender-0.1.45-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 14.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from visualization) (1.21.6)\n",
            "Collecting autolab-core\n",
            "  Downloading autolab_core-1.1.1-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 57.5 MB/s \n",
            "\u001b[?25hCollecting trimesh[easy]\n",
            "  Downloading trimesh-3.16.4-py3-none-any.whl (663 kB)\n",
            "\u001b[K     |████████████████████████████████| 663 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from visualization) (3.2.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from visualization) (2.9.0)\n",
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 72.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (4.6.0.66)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (1.2.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 71.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (1.0.2)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (1.7.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from autolab-core->visualization) (0.18.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->visualization) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->visualization) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->visualization) (1.15.0)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.7/dist-packages (from multiprocess->autolab-core->visualization) (0.3.6)\n",
            "Collecting freetype-py\n",
            "  Downloading freetype_py-2.3.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (978 kB)\n",
            "\u001b[K     |████████████████████████████████| 978 kB 63.6 MB/s \n",
            "\u001b[?25hCollecting pyglet>=1.4.10\n",
            "  Downloading pyglet-2.0.0-py3-none-any.whl (966 kB)\n",
            "\u001b[K     |████████████████████████████████| 966 kB 65.7 MB/s \n",
            "\u001b[?25hCollecting PyOpenGL==3.1.0\n",
            "  Downloading PyOpenGL-3.1.0.zip (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pyrender->visualization) (2.6.3)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 68.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->autolab-core->visualization) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->autolab-core->visualization) (2021.11.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autolab-core->visualization) (3.1.0)\n",
            "Collecting rtree\n",
            "  Downloading Rtree-1.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 65.3 MB/s \n",
            "\u001b[?25hCollecting pycollada\n",
            "  Downloading pycollada-0.7.2.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 71.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (4.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (2.23.0)\n",
            "Collecting svg.path\n",
            "  Downloading svg.path-6.2-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (3.0.4)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.0.4)\n",
            "Collecting pyglet>=1.4.10\n",
            "  Downloading pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 62.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (57.4.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (4.3.3)\n",
            "Collecting mapbox-earcut\n",
            "  Downloading mapbox_earcut-1.0.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: shapely in /usr/local/lib/python3.7/dist-packages (from trimesh[easy]->visualization) (1.8.5.post1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (5.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (4.13.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->trimesh[easy]->visualization) (22.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->trimesh[easy]->visualization) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->trimesh[easy]->visualization) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->trimesh[easy]->visualization) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->trimesh[easy]->visualization) (2022.9.24)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->trimesh[easy]->visualization) (1.2.1)\n",
            "Building wheels for collected packages: PyOpenGL, pycollada\n",
            "  Building wheel for PyOpenGL (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyOpenGL: filename=PyOpenGL-3.1.0-py3-none-any.whl size=1745210 sha256=8efe0f1e3d8a60941febb544b60f020a5ae8a3998dfe77a98d3ceef916b8c390\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/83/cb/af51a0c06c33d08537b941bbfc87469e8a3c68d05f77a6a212\n",
            "  Building wheel for pycollada (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycollada: filename=pycollada-0.7.2-py3-none-any.whl size=127026 sha256=3ba057661eeb0f79a402d62c4752d7ae6ae7f9485379711c2b4ad95a576e473b\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/0b/be/6cad4a774484615180dc30a68ff3635fbc36cccf489bcd6543\n",
            "Successfully built PyOpenGL pycollada\n",
            "Installing collected packages: ruamel.yaml.clib, xxhash, trimesh, svg.path, setproctitle, ruamel.yaml, rtree, PyOpenGL, pyglet, pycollada, multiprocess, mapbox-earcut, freetype-py, colorlog, pyrender, autolab-core, visualization\n",
            "  Attempting uninstall: PyOpenGL\n",
            "    Found existing installation: PyOpenGL 3.1.6\n",
            "    Uninstalling PyOpenGL-3.1.6:\n",
            "      Successfully uninstalled PyOpenGL-3.1.6\n",
            "Successfully installed PyOpenGL-3.1.0 autolab-core-1.1.1 colorlog-6.7.0 freetype-py-2.3.0 mapbox-earcut-1.0.0 multiprocess-0.70.14 pycollada-0.7.2 pyglet-1.5.27 pyrender-0.1.45 rtree-1.0.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 setproctitle-1.3.2 svg.path-6.2 trimesh-3.16.4 visualization-1.0.0 xxhash-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from visualization import *\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "import os\n",
        "\n",
        "\n",
        "def ShowAttentionV1(model,image_path):\n",
        "    file_list = os.listdir(image_path)\n",
        "    file_list.sort()\n",
        "    for filename in file_list:\n",
        "        print (filename)\n",
        "        filepath=image_path+filename\n",
        "        image=load_image(filepath)\n",
        "        image = image/255.0\n",
        "        gender=1.0\n",
        "        gender=np.asarray(gender)\n",
        "        gender=np.expand_dims(gender,axis=0)\n",
        "        layer=K.function([model.layers[0].input],[model.layers[196].output])\n",
        "        FeatureMap=layer([image,gender])[0]\n",
        "        print (FeatureMap.shape)\n",
        "        FeatureMap = np.squeeze(FeatureMap, axis=0)\n",
        "        FeatureMap = np.abs(FeatureMap)\n",
        "        heatmap = np.mean(FeatureMap,axis=2)\n",
        "        heatmap = heatmap/np.max(heatmap)\n",
        "        heatmap = np.uint8(255*heatmap)\n",
        "        print (heatmap.shape)\n",
        "        heatmap = cv2.applyColorMap(heatmap,cv2.COLORMAP_JET)\n",
        "        SaveImg(filename,filepath,heatmap)\n",
        "    print ('********** Done ***********')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def GAPAttention(model,weights,image_path):\n",
        "    file_list = os.listdir(image_path)\n",
        "    file_list.sort()\n",
        "    for filename in file_list:\n",
        "        filepath=image_path+filename\n",
        "        print (filepath)\n",
        "        image=load_image(filepath)\n",
        "        print(\"gpa\")\n",
        "        print(image.shape)\n",
        "        image = image/255.0\n",
        "        gender=1.0\n",
        "        gender=np.asarray(gender)\n",
        "        gender=np.expand_dims(gender,axis=0)\n",
        "        print(\"ok\")\n",
        "        layer=K.function([model.layers[0].input],[model.layers[1].get_output_at(-1),model.layers[-1].output])\n",
        "        print(\"ok_1\")\n",
        "        GAP,prediction=layer([image])\n",
        "        print(\"ok_2\")\n",
        "        GAP=np.squeeze(GAP,axis=0)\n",
        "        print(\"ok_3\")\n",
        "        print (GAP.shape)\n",
        "        print(\"ok_4\")\n",
        "        index = np.argmax(prediction)\n",
        "        print(\"ok_5\")\n",
        "        print (index)\n",
        "       # weight = weights[:,index]\n",
        "        weight =np.mean(weights[:,index-5:index+5],axis=1)\n",
        "        heatmap = np.zeros((GAP.shape[0],GAP.shape[1]))\n",
        "        for k in range(GAP.shape[2]):\n",
        "            heatmap = heatmap + weight[k]*GAP[:,:,k]\n",
        "        heatmap = heatmap/np.max(heatmap)\n",
        "        heatmap = np.uint8(255*heatmap)\n",
        "        print (heatmap.shape)\n",
        "        heatmap = cv2.applyColorMap(heatmap,cv2.COLORMAP_JET)\n",
        "        SaveImg(filename,filepath,heatmap)\n",
        "    print ('********** Done ***********')\n",
        "\n",
        "\n",
        "\n",
        "def SaveImg(filename,filepath,heatmap):\n",
        "    img = cv2.imread(filepath)\n",
        "    heatmap = cv2.resize(heatmap,(img.shape[1],img.shape[0]))\n",
        "    AttentionImg =0.5* heatmap + img\n",
        "    cv2.imwrite('/content/heat'+filename,heatmap)\n",
        "    cv2.imwrite('/content/attention'+filename,AttentionImg)\n",
        "\n",
        "\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path)\n",
        "    print(img.shape)\n",
        "    img = cv2.resize(img,(300,300))\n",
        "    print(img.shape)\n",
        "    x = np.asarray(img, dtype=np.float32)\n",
        "   # img = image.load_img(path, target_size=(448, 448))\n",
        "   # print (img.shape)\n",
        "   # x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    return x\n",
        "\n",
        "def TestMAE(model,test_data,test_label,test_gender):\n",
        "    test_gender = np.array(test_gender)\n",
        "    test_gender = np.expand_dims(test_gender,axis=1)\n",
        "    layer=K.function([model.layers[0].input,model.layers[3].input],[model.layers[-1].output])\n",
        "    predictions=layer([test_data,test_gender])\n",
        "    predictions = np.array(predictions)\n",
        "    predictions = np.squeeze(predictions,axis=0)\n",
        "    print (predictions.shape)\n",
        "    predict_label = np.argmax(predictions,axis=1)\n",
        "    test_label = np.argmax(test_label,axis=1)\n",
        "    print (predict_label)\n",
        "    print (test_label)\n",
        "    TestMAE = np.mean(np.abs(predict_label-test_label))\n",
        "    return TestMAE\n",
        "\n",
        "\n",
        "\n",
        "def DataAugment(x_train):\n",
        "    x_train_Aug = np.zeros(x_train.shape)\n",
        "    for i in range(x_train.shape[0]):\n",
        "        for j in range(3):\n",
        "            img = x_train[i,:,:,j]\n",
        "            img = RandomMask(img)\n",
        "            img = RandomMask(img)\n",
        "            if np.random.random()>-1:\n",
        "                x_train_Aug[i,:,:,j]=img \n",
        "            else:\n",
        "                x_train_Aug[i,:,:,j]=x_train[i,:,:,j]\n",
        "    return x_train_Aug\n",
        "\n",
        "\n",
        "def RandomMask(img):\n",
        "    m,n=img.shape\n",
        "    m=int(m/6)\n",
        "    n=int(n/6)\n",
        "    i,j = np.random.randint(0,6,2)\n",
        "    img[i*m:(i+1)*m,j*n:(j+1)*n]=np.random.random()\n",
        "    return img\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z4Db80-IClx",
        "outputId": "57d864b8-c6ae-4e42-f0f9-2647a71f389e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to import geometry msgs in rigid_transformations.py.\n",
            "WARNING:root:Failed to import ros dependencies in rigid_transforms.py\n",
            "WARNING:root:autolab_core not installed as catkin package, RigidTransform ros methods will be unavailable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.xception import Xception\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Flatten, Dense, Input, Reshape, Lambda,Conv2D\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "#from func_utils import *\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
        "#os.environ['OMP_NUM_THREADS']='6'\n",
        "batch_size = 32\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "Ko7j7X9lIrEk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model11 = ResNet50(weights='imagenet', include_top=False)"
      ],
      "metadata": {
        "id": "HALtGDlZI3sO",
        "outputId": "ae99f922-7808-45ce-cfb3-ab61795b9418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model11.trainable = True\n",
        "print(\"Number of layers in the base model: \", len(base_model11.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model11.layers[:fine_tune_at]:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "lvd4LGhaI_rT",
        "outputId": "ec9e0bc7-5951-4bf5-b581-9eb79c58c976",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in the base model:  175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def channel_attention(input_feature):\n",
        "  kernel_initializer = tf.keras.initializers.variance_scaling()\n",
        "  bias_initializer = tf.constant_initializer(value=0.0)\n",
        "  channel = input_feature.get_shape()[-1]\n",
        "  ratio=8\n",
        "  avg_pool = tf.reduce_mean(input_feature, axis=[1,2], keepdims=True)\n",
        "  assert avg_pool.get_shape()[1:] == (1,1,channel)\n",
        "  avg_pool =Dense(\n",
        "                                 units=channel//ratio,\n",
        "                                 activation=tf.nn.relu,\n",
        "                                 kernel_initializer=kernel_initializer,\n",
        "                                 bias_initializer=bias_initializer) (avg_pool)\n",
        "  assert avg_pool.get_shape()[1:] == (1,1,channel//ratio)\n",
        "  avg_pool = Dense(\n",
        "                                 units=channel,                             \n",
        "                                 kernel_initializer=kernel_initializer,\n",
        "                                 bias_initializer=bias_initializer)  (avg_pool)  \n",
        "  assert avg_pool.get_shape()[1:] == (1,1,channel)\n",
        "\n",
        "  max_pool = tf.reduce_max(input_feature, axis=[1,2], keepdims=True)    \n",
        "  assert max_pool.get_shape()[1:] == (1,1,channel)\n",
        "  max_pool = Dense(\n",
        "                                 units=channel//ratio,\n",
        "                                 activation=tf.nn.relu) (max_pool)  \n",
        "  assert max_pool.get_shape()[1:] == (1,1,channel//ratio)\n",
        "  max_pool = Dense(\n",
        "                                 units=channel)  (max_pool)\n",
        "  assert max_pool.get_shape()[1:] == (1,1,channel)\n",
        "\n",
        "  scale = tf.keras.activations.sigmoid(avg_pool + max_pool) \n",
        "  return input_feature * scale\n"
      ],
      "metadata": {
        "id": "wRw98DsAJSiN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spatial_attention(input_feature):\n",
        "  kernel_size = 7\n",
        "  kernel_initializer = tf.keras.initializers.variance_scaling()\n",
        "\n",
        "  avg_pool = tf.reduce_mean(input_feature, axis=[3], keepdims=True)\n",
        "  assert avg_pool.get_shape()[-1] == 1\n",
        "  max_pool = tf.reduce_max(input_feature, axis=[3], keepdims=True)\n",
        "  assert max_pool.get_shape()[-1] == 1\n",
        "  concat = tf.concat([avg_pool,max_pool], 3)\n",
        "  assert concat.get_shape()[-1] == 2\n",
        "\n",
        "  concat = Conv2D(\n",
        "                              filters=1,\n",
        "                              kernel_size=[kernel_size,kernel_size],\n",
        "                              strides=[1,1],\n",
        "                              padding=\"same\",\n",
        "                              activation=None,\n",
        "                              kernel_initializer=kernel_initializer,\n",
        "                              use_bias=False)(concat)\n",
        "  assert concat.get_shape()[-1] == 1\n",
        "  concat = tf.keras.activations.sigmoid(concat)\n",
        "    \n",
        "  return input_feature * concat"
      ],
      "metadata": {
        "id": "lEvkPbShNQlx"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input11 = Input(shape=(300,300,3),name='input1')\n",
        "input_gender11 = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output11 = base_model11(input11)\n",
        "gender_embedding11=Dense(16)(input_gender11)\n",
        "print (K.int_shape(output11))\n",
        "x11=channel_attention(output11)\n",
        "print (K.int_shape(x11))\n",
        "x11=spatial_attention(x11)\n",
        "print (K.int_shape(x11))\n",
        "x11 = keras.layers.MaxPooling2D(pool_size=(8,8))(x11)\n",
        "print (K.int_shape(x11))\n",
        "x11=Flatten()(x11)\n",
        "f11 = keras.layers.Concatenate(axis=1)([x11,gender_embedding11])\n",
        "print (K.int_shape(f11)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "prediction11 = Dense(240)(x11)\n",
        "\n",
        "model11 = Model(inputs=[input11,input_gender11], outputs=prediction11)\n",
        "for i,layer in enumerate(model11.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0003,beta_1=0.9,beta_2=0.999)\n",
        "model11.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])"
      ],
      "metadata": {
        "id": "SQPE7PrOJFrs",
        "outputId": "69ac45db-d163-40fa-feec-18bd2ba98b24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 10, 10, 2048)\n",
            "(None, 10, 10, 2048)\n",
            "(None, 10, 10, 2048)\n",
            "(None, 1, 1, 2048)\n",
            "(None, 2064)\n",
            "0 input1\n",
            "1 resnet50\n",
            "2 tf.math.reduce_mean_13\n",
            "3 tf.math.reduce_max_6\n",
            "4 dense_32\n",
            "5 dense_34\n",
            "6 dense_33\n",
            "7 dense_35\n",
            "8 tf.__operators__.add_4\n",
            "9 tf.math.sigmoid_6\n",
            "10 tf.math.multiply_6\n",
            "11 tf.math.reduce_mean_14\n",
            "12 tf.math.reduce_max_7\n",
            "13 tf.concat_2\n",
            "14 conv2d_2\n",
            "15 tf.math.sigmoid_7\n",
            "16 tf.math.multiply_7\n",
            "17 max_pooling2d_3\n",
            "18 flatten_3\n",
            "19 input2\n",
            "20 dense_36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(model11.trainable_variables)"
      ],
      "metadata": {
        "id": "hbDJTeMPOMuL",
        "outputId": "48e6714b-3cbb-4f04-d639-0b669b3ae9be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "history11=model11.fit([x_train,gender_train],y_train,batch_size=batch_size,epochs=100,verbose=1,validation_data=([x_valid,gender_valid],y_valid), callbacks = [checkpoint])"
      ],
      "metadata": {
        "id": "CqwBHvm_WBrE",
        "outputId": "aca3a43a-f062-4096-e34a-67d6a17f5a37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 6s 1s/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2052 - val_MAE: 0.2052\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2052 - val_MAE: 0.2052\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 1s 375ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2052 - val_MAE: 0.2052\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2052 - val_MAE: 0.2052\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2052 - val_MAE: 0.2052\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 1s 377ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2095 - val_MAE: 0.2095\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 1s 302ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2187 - val_MAE: 0.2187\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 1s 376ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2167 - val_MAE: 0.2167\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2136 - val_MAE: 0.2136\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 1s 304ms/step - loss: 0.2013 - MAE: 0.2013 - val_loss: 0.2113 - val_MAE: 0.2113\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 1s 305ms/step - loss: 0.2014 - MAE: 0.2014 - val_loss: 0.2095 - val_MAE: 0.2095\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 1s 321ms/step - loss: 0.2014 - MAE: 0.2014 - val_loss: 0.2148 - val_MAE: 0.2148\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 1s 322ms/step - loss: 0.2014 - MAE: 0.2014 - val_loss: 0.2174 - val_MAE: 0.2174\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 1s 326ms/step - loss: 0.2039 - MAE: 0.2039 - val_loss: 0.2136 - val_MAE: 0.2136\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 1s 323ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2430 - val_MAE: 0.2430\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 1s 332ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3104 - val_MAE: 0.3104\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 1s 318ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.4397 - val_MAE: 0.4397\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 1s 328ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.6700 - val_MAE: 0.6700\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 1s 328ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.9416 - val_MAE: 0.9416\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 1s 394ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.1129 - val_MAE: 1.1129\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 1s 331ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.2995 - val_MAE: 1.2995\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 1s 323ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.1505 - val_MAE: 1.1505\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 1s 323ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.0620 - val_MAE: 1.0620\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 1s 318ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 1.0010 - val_MAE: 1.0010\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 1s 794ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.9362 - val_MAE: 0.9362\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 1s 320ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.8615 - val_MAE: 0.8615\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 1s 305ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.7782 - val_MAE: 0.7782\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 1s 309ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.7029 - val_MAE: 0.7029\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 1s 301ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.6248 - val_MAE: 0.6248\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 1s 303ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.5589 - val_MAE: 0.5589\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 1s 302ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.5022 - val_MAE: 0.5022\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 1s 313ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.4454 - val_MAE: 0.4454\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 1s 309ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3959 - val_MAE: 0.3959\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3521 - val_MAE: 0.3521\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 1s 305ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.3163 - val_MAE: 0.3163\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 1s 304ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2779 - val_MAE: 0.2779\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2655 - val_MAE: 0.2655\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 1s 303ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2496 - val_MAE: 0.2496\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 1s 310ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2281 - val_MAE: 0.2281\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 1s 309ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2411 - val_MAE: 0.2411\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 1s 418ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2333 - val_MAE: 0.2333\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 1s 311ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2337 - val_MAE: 0.2337\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 1s 301ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2302 - val_MAE: 0.2302\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 1s 305ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2252 - val_MAE: 0.2252\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 1s 308ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2205 - val_MAE: 0.2205\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 1s 308ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2161 - val_MAE: 0.2161\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 1s 302ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2119 - val_MAE: 0.2119\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 1s 393ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2089 - val_MAE: 0.2089\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2074 - val_MAE: 0.2074\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 1s 313ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2064 - val_MAE: 0.2064\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 1s 311ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2060 - val_MAE: 0.2060\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2057 - val_MAE: 0.2057\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 1s 386ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2054 - val_MAE: 0.2054\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 1s 302ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 1s 673ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 1s 383ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 1s 303ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2059 - val_MAE: 0.2059\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 1s 334ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2128 - val_MAE: 0.2128\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2204 - val_MAE: 0.2204\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 1s 302ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2055 - val_MAE: 0.2055\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 1s 297ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 1s 301ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2057 - val_MAE: 0.2057\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2061 - val_MAE: 0.2061\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 1s 297ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2062 - val_MAE: 0.2062\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2062 - val_MAE: 0.2062\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 1s 355ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2062 - val_MAE: 0.2062\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 1s 359ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2060 - val_MAE: 0.2060\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 1s 348ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2057 - val_MAE: 0.2057\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 1s 328ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2055 - val_MAE: 0.2055\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 1s 302ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 1s 340ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 1s 401ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 1s 383ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 1s 385ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 1s 372ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 1s 368ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 1s 405ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 2s 2s/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 1s 425ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 1s 385ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 1s 375ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 1s 307ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 1s 297ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history11.history['loss']\n",
        "val_loss = history11.history['val_loss']\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h8Dne0D5WF1W",
        "outputId": "777c6a3a-1eea-4f27-d685-d2904ad877cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model11.summary\n"
      ],
      "metadata": {
        "id": "Fs9_rRAIWb-X",
        "outputId": "8c203d4e-b76d-4167-e312-05841f7b287d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Model.summary of <keras.engine.functional.Functional object at 0x7f10ae22f210>>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input11 = Input(shape=(300,300,3),name='input1')\n",
        "input_gender11 = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output11 = base_model11(input11)\n",
        "gender_embedding11=Dense(16)(input_gender11)\n",
        "print (K.int_shape(output11))\n",
        "x11=channel_attention(output11)\n",
        "print (K.int_shape(x11))\n",
        "#x11=spatial_attention(x11)\n",
        "#print (K.int_shape(x11))\n",
        "x11 = keras.layers.MaxPooling2D(pool_size=(8,8))(x11)\n",
        "print (K.int_shape(x11))\n",
        "x11=Flatten()(x11)\n",
        "f11 = keras.layers.Concatenate(axis=1)([x11,gender_embedding11])\n",
        "print (K.int_shape(f11)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "prediction11 = Dense(240)(x11)\n",
        "\n",
        "model12 = Model(inputs=[input11,input_gender11], outputs=prediction11)\n",
        "for i,layer in enumerate(model11.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0003,beta_1=0.9,beta_2=0.999)\n",
        "model12.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])"
      ],
      "metadata": {
        "id": "3ubw8IONWnxf",
        "outputId": "ecc716dd-a398-45b6-e541-573623c86717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 10, 10, 2048)\n",
            "(None, 10, 10, 2048)\n",
            "(None, 1, 1, 2048)\n",
            "(None, 2064)\n",
            "0 input1\n",
            "1 resnet50\n",
            "2 tf.math.reduce_mean_13\n",
            "3 tf.math.reduce_max_6\n",
            "4 dense_32\n",
            "5 dense_34\n",
            "6 dense_33\n",
            "7 dense_35\n",
            "8 tf.__operators__.add_4\n",
            "9 tf.math.sigmoid_6\n",
            "10 tf.math.multiply_6\n",
            "11 tf.math.reduce_mean_14\n",
            "12 tf.math.reduce_max_7\n",
            "13 tf.concat_2\n",
            "14 conv2d_2\n",
            "15 tf.math.sigmoid_7\n",
            "16 tf.math.multiply_7\n",
            "17 max_pooling2d_3\n",
            "18 flatten_3\n",
            "19 input2\n",
            "20 dense_36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "history12=model12.fit([x_train,gender_train],y_train,batch_size=batch_size,epochs=100,verbose=1,validation_data=([x_valid,gender_valid],y_valid), callbacks = [checkpoint])"
      ],
      "metadata": {
        "id": "XqFXCCw-YTW0",
        "outputId": "32a9feb0-6951-4f87-d5fc-cc570792e519",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 6s 1s/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2497 - val_MAE: 0.2497\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 1s 286ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2445 - val_MAE: 0.2445\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 1s 286ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2398 - val_MAE: 0.2398\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2359 - val_MAE: 0.2359\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2325 - val_MAE: 0.2325\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 1s 312ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2293 - val_MAE: 0.2293\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 1s 312ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2265 - val_MAE: 0.2265\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 1s 286ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2241 - val_MAE: 0.2241\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2220 - val_MAE: 0.2220\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2200 - val_MAE: 0.2200\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2183 - val_MAE: 0.2183\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2169 - val_MAE: 0.2169\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2157 - val_MAE: 0.2157\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 1s 286ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2147 - val_MAE: 0.2147\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2138 - val_MAE: 0.2138\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2128 - val_MAE: 0.2128\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2120 - val_MAE: 0.2120\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2112 - val_MAE: 0.2112\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2106 - val_MAE: 0.2106\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2100 - val_MAE: 0.2100\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2094 - val_MAE: 0.2094\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2089 - val_MAE: 0.2089\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2085 - val_MAE: 0.2085\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2081 - val_MAE: 0.2081\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2078 - val_MAE: 0.2078\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2074 - val_MAE: 0.2074\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 1s 375ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2072 - val_MAE: 0.2072\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2069 - val_MAE: 0.2069\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2067 - val_MAE: 0.2067\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 1s 752ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2065 - val_MAE: 0.2065\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2064 - val_MAE: 0.2064\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2062 - val_MAE: 0.2062\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2061 - val_MAE: 0.2061\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2060 - val_MAE: 0.2060\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 1s 373ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2060 - val_MAE: 0.2060\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2059 - val_MAE: 0.2059\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 1s 319ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2058 - val_MAE: 0.2058\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 1s 385ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2058 - val_MAE: 0.2058\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 1s 386ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2057 - val_MAE: 0.2057\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 1s 388ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2057 - val_MAE: 0.2057\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 1s 322ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2056 - val_MAE: 0.2056\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 1s 392ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2056 - val_MAE: 0.2056\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 1s 391ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2056 - val_MAE: 0.2056\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 1s 314ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2055 - val_MAE: 0.2055\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 1s 308ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2055 - val_MAE: 0.2055\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2055 - val_MAE: 0.2055\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2055 - val_MAE: 0.2055\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2054 - val_MAE: 0.2054\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 1s 309ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2054 - val_MAE: 0.2054\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 1s 324ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2054 - val_MAE: 0.2054\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 1s 340ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2054 - val_MAE: 0.2054\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 1s 412ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2054 - val_MAE: 0.2054\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 1s 353ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2054 - val_MAE: 0.2054\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 1s 329ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 1s 326ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 1s 327ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 1s 390ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 2s 2s/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 1s 319ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 1s 374ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 1s 373ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 1s 303ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 1s 302ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 1s 708ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 1s 373ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 1s 297ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 1s 336ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 1s 307ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 1s 314ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 1s 305ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history12.history['loss']\n",
        "val_loss = history12.history['val_loss']\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-q9EnTKnYZvs",
        "outputId": "aab21f1e-fff0-4641-e963-5ee4a06abc94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input11 = Input(shape=(300,300,3),name='input1')\n",
        "input_gender11 = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output11 = base_model11(input11)\n",
        "gender_embedding11=Dense(16)(input_gender11)\n",
        "print (K.int_shape(output11))\n",
        "#x11=channel_attention(output11)\n",
        "#print (K.int_shape(x11))\n",
        "x11=spatial_attention(output11)\n",
        "print (K.int_shape(x11))\n",
        "x11 = keras.layers.MaxPooling2D(pool_size=(8,8))(x11)\n",
        "print (K.int_shape(x11))\n",
        "x11=Flatten()(x11)\n",
        "f11 = keras.layers.Concatenate(axis=1)([x11,gender_embedding11])\n",
        "print (K.int_shape(f11)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "prediction11 = Dense(240)(x11)\n",
        "\n",
        "model13 = Model(inputs=[input11,input_gender11], outputs=prediction11)\n",
        "for i,layer in enumerate(model11.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0003,beta_1=0.9,beta_2=0.999)\n",
        "model13.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])"
      ],
      "metadata": {
        "id": "LV5z1LWFZcU1",
        "outputId": "f0b44a9c-5f6f-4168-bf5f-96ee4504e8f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 10, 10, 2048)\n",
            "(None, 10, 10, 2048)\n",
            "(None, 1, 1, 2048)\n",
            "(None, 2064)\n",
            "0 input1\n",
            "1 resnet50\n",
            "2 tf.math.reduce_mean_13\n",
            "3 tf.math.reduce_max_6\n",
            "4 dense_32\n",
            "5 dense_34\n",
            "6 dense_33\n",
            "7 dense_35\n",
            "8 tf.__operators__.add_4\n",
            "9 tf.math.sigmoid_6\n",
            "10 tf.math.multiply_6\n",
            "11 tf.math.reduce_mean_14\n",
            "12 tf.math.reduce_max_7\n",
            "13 tf.concat_2\n",
            "14 conv2d_2\n",
            "15 tf.math.sigmoid_7\n",
            "16 tf.math.multiply_7\n",
            "17 max_pooling2d_3\n",
            "18 flatten_3\n",
            "19 input2\n",
            "20 dense_36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "history13=model13.fit([x_train,gender_train],y_train,batch_size=batch_size,epochs=100,verbose=1,validation_data=([x_valid,gender_valid],y_valid), callbacks = [checkpoint])"
      ],
      "metadata": {
        "id": "LOPhLwqQZ362",
        "outputId": "4eeaab10-78c7-4026-8b45-02c24658ce27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 6s 1s/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 1s 284ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 1s 287ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 1s 285ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2012 - MAE: 0.2012 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 1s 289ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 1s 304ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 1s 313ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 1s 306ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 1s 308ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 1s 315ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 1s 304ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 1s 310ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 1s 679ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 1s 304ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 1s 328ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 1s 311ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 1s 321ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 1s 314ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 1s 321ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 1s 297ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 1s 336ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 1s 317ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 1s 325ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 1s 342ms/step - loss: 0.2011 - MAE: 0.2011 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 1s 437ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 1s 411ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 1s 375ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 1s 322ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 1s 318ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 1s 321ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 1s 389ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 1s 363ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 1s 301ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 1s 299ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 1s 693ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 1s 297ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 1s 380ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 1s 297ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 1s 296ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 1s 298ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 1s 301ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 1s 294ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 1s 295ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 1s 307ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 1s 391ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 1s 309ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 1s 318ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 1s 313ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 1s 305ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 1s 311ms/step - loss: 0.2010 - MAE: 0.2010 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 1s 307ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 1s 309ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 1s 685ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 1s 293ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 1s 290ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 1s 288ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 1s 370ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 1s 300ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 1s 292ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.2009 - MAE: 0.2009 - val_loss: 0.2053 - val_MAE: 0.2053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history13.history['loss']\n",
        "val_loss = history13.history['val_loss']\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wk90rLLUaE-5",
        "outputId": "46346011-03a3-4d36-8f31-f9f6a0b3da25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4l8zN8pEaYCJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}